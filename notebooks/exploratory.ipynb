{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_fname = \"./data/goblet_book.txt\"\n",
    "with open(book_fname, 'r') as book:\n",
    "    book_data = book.read()\n",
    "len(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = book_data.split()\n",
    "chars = [[*word] for word in word_list]\n",
    "max_len = max(len(word) for word in chars)\n",
    "for wordl in chars:\n",
    "    while len(wordl) < max_len:\n",
    "        wordl.append(' ')\n",
    "chars = np.array(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = list(np.unique(chars))\n",
    "unique_chars.append('\\n')\n",
    "unique_chars.append('\\t')\n",
    "K = len(unique_chars)  # dimensionality of the input and output vectors\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "for idx, char in enumerate(unique_chars):\n",
    "    char_to_ind[char] = idx\n",
    "    ind_to_char[idx] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100  # dimensionality of the hidden state\n",
    "eta = 0.1  # learning rate\n",
    "seq_length = 25  # length of input sequences used during training\n",
    "epsilon = 1e-8  # for AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 0.01\n",
    "RNN = {'b': torch.zeros((m, 1), dtype=torch.double), 'c': torch.zeros((K, 1), dtype=torch.double), 'U': torch.normal(0.0, sig, (m, K), dtype=torch.double), 'W': torch.normal(0.0, sig, (m, m), dtype=torch.double), 'V': torch.normal(0.0, sig, (K, m), dtype=torch.double), 'h0': torch.zeros((m, 1), dtype=torch.double)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(char):\n",
    "    oh = [0]*K\n",
    "    oh[char_to_ind[char]] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetize_seq(rnn, h0, x0, n):\n",
    "    t, ht, xt = 0, h0, x0\n",
    "    indexes = []\n",
    "    while t < n:\n",
    "        xt = xt.reshape((K, 1))\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b']\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c']\n",
    "        pt = F.softmax(ot, dim=0)\n",
    "        cp = torch.cumsum(pt, dim=0)\n",
    "        a = torch.rand(1)\n",
    "        ixs = torch.where(cp - a > 0)\n",
    "        ii = ixs[0][0].item()\n",
    "        indexes.append(ii)\n",
    "        xt = torch.zeros((K, 1), dtype=torch.double)\n",
    "        xt[ii, 0] = 1\n",
    "        t += 1\n",
    "    Y = []\n",
    "    for idx in indexes:\n",
    "        oh = [0]*K\n",
    "        oh[idx] = 1\n",
    "        Y.append(oh)\n",
    "    Y = torch.tensor(Y).t()\n",
    "    \n",
    "    s = ''\n",
    "    for i in range(Y.shape[1]):\n",
    "        idx = torch.where(Y[:, i] == 1)[0].item()\n",
    "        s += ind_to_char[idx]\n",
    "    \n",
    "    return Y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chars = book_data[:seq_length]\n",
    "Y_chars = book_data[1:seq_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_string(chars):\n",
    "    M = []\n",
    "    for i in range(len(chars)):\n",
    "        M.append(encode_char(chars[i]))\n",
    "    M = torch.tensor(M, dtype=torch.double).t()\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encode_string(X_chars)\n",
    "Y = encode_string(Y_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0, s0 = synthetize_seq(RNN, RNN['h0'], X[:,0], 200)\n",
    "print(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(rnn, X, hprev):\n",
    "    ht = hprev.clone()\n",
    "    indexes = []\n",
    "    P = torch.zeros((K, seq_length), dtype=torch.double)\n",
    "    A = torch.zeros((m, seq_length), dtype=torch.double)\n",
    "    H = torch.zeros((m, seq_length), dtype=torch.double)\n",
    "    for i in range(seq_length):\n",
    "        xt = X[:, i].reshape((K, 1))\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b']\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c']\n",
    "        pt = F.softmax(ot, dim=0)\n",
    "\n",
    "        H[:, i] = ht.squeeze()\n",
    "        P[:, i] = pt.squeeze()\n",
    "        A[:, i] = at.squeeze()\n",
    "        cp = torch.cumsum(pt, dim=0)\n",
    "        a = torch.rand(1)\n",
    "        ixs = torch.where(cp - a > 0)\n",
    "        ii = ixs[0][0].item()\n",
    "        indexes.append(ii)\n",
    "\n",
    "    Y_pred = []\n",
    "    for idx in indexes:\n",
    "        oh = [0]*K\n",
    "        oh[idx] = 1\n",
    "        Y_pred.append(oh)\n",
    "    Y_pred = torch.tensor(Y_pred, dtype=torch.double).t()\n",
    "\n",
    "    s_pred = ''\n",
    "    for i in range(Y_pred.shape[1]):\n",
    "        idx = torch.where(Y_pred[:, i] == 1)[0].item()\n",
    "        s_pred += ind_to_char[idx]\n",
    "\n",
    "    return s_pred, Y_pred, A, H, P, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pred, Y_pred, A, H, P, ht = forward(RNN, X, RNN['h0'])\n",
    "print(s_pred, Y_pred.shape, A.shape, H.shape, P.shape, ht.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, P):\n",
    "    log_probs = torch.log(P)\n",
    "    cross_entropy = -torch.sum(Y * log_probs)\n",
    "    loss = cross_entropy.item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_loss(Y, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(rnn, X, Y, A, H, P, hprev):\n",
    "    dA = torch.zeros_like(A)\n",
    "    dH = torch.zeros_like(H)\n",
    "\n",
    "    G = -(Y - P)\n",
    "    dV = torch.matmul(G, H.t())\n",
    "    dhtau = torch.matmul(G[:, -1], rnn['V'])\n",
    "    datau = (1 - torch.pow(torch.tanh(A[:, -1]), 2)) * dhtau\n",
    "    dH[:, -1] = dhtau.squeeze()\n",
    "    dA[:, -1] = datau.squeeze()\n",
    "\n",
    "    for i in range(seq_length - 2, -1, -1):\n",
    "        dht = torch.matmul(G[:, i], rnn['V']) + torch.matmul(dA[:, i+1].reshape(1, -1), rnn['W'])\n",
    "        dat = (1 - torch.pow(torch.tanh(A[:, i]), 2)) * dht\n",
    "        dH[:, i] = dht.squeeze()\n",
    "        dA[:, i] = dat.squeeze()\n",
    "\n",
    "    Hd = torch.cat((hprev, H[:, :-1]), dim=1)\n",
    "    dW = torch.matmul(dA, Hd.t())\n",
    "    dU = torch.matmul(dA, X.t())\n",
    "    dc = G.sum(1).reshape((-1, 1))\n",
    "    db = dA.sum(1).reshape((-1, 1))\n",
    "    grads = {'U': dU, 'W': dW, 'V': dV, 'c': dc, 'b': db}\n",
    "    grads_clamped = {k: torch.clamp(v, min=-5.0, max=5.0) for (k,v) in grads.items()}\n",
    "    return grads, grads_clamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads, grads_clamped = backward(RNN, X, Y, A, H, P, ht)\n",
    "print(*list(map(lambda v: grads_clamped[v].shape, grads_clamped)), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradNum(X, Y, param_name, rnn, h=1e-4):\n",
    "    \"\"\"\n",
    "    Compute the numerical gradient of the rnn's parameter specified by param_name.\n",
    "    \"\"\"\n",
    "    grad = torch.zeros_like(rnn[param_name])\n",
    "    hprev = torch.zeros(rnn['W'].shape[0], 1, dtype=torch.double)\n",
    "    n = torch.numel(rnn[param_name])\n",
    "    \n",
    "    for i in range(n):\n",
    "        old_val = rnn[param_name].view(-1)[i].item()\n",
    "        rnn[param_name].view(-1)[i] = old_val - h\n",
    "        s_pred, Y_pred, A, H, P, ht = forward(rnn, X, hprev)\n",
    "        l1 = compute_loss(Y, P)\n",
    "        \n",
    "        rnn[param_name].view(-1)[i] = old_val + h\n",
    "        s_pred, Y_pred, A, H, P, ht = forward(rnn, X, hprev)\n",
    "        l2 = compute_loss(Y, P)\n",
    "        \n",
    "        grad.view(-1)[i] = (l2 - l1) / (2 * h)\n",
    "        rnn[param_name].view(-1)[i] = old_val  # Reset to original value\n",
    "\n",
    "    return grad\n",
    "\n",
    "def ComputeGradsNum(X, Y, rnn, h=1e-4):\n",
    "    num_grads = {}\n",
    "    for param_name in rnn:\n",
    "        if param_name != 'h0':\n",
    "            print('Computing numerical gradient for')\n",
    "            print(f'Field name: {param_name}')\n",
    "            num_grads[param_name] = ComputeGradNum(X, Y, param_name, rnn, h)\n",
    "    return num_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "num_grads = ComputeGradsNum(X, Y, RNN, 1e-4)\n",
    "print(\"-------- Gradient validation --------\")\n",
    "print(\"Max diff for gradient of b:\", torch.max(torch.abs(num_grads['b'] - grads['b'])).item())\n",
    "print(\"Max diff for gradient of c:\", torch.max(torch.abs(num_grads['c'] - grads['c'])).item())\n",
    "print(\"Max diff for gradient of W:\", torch.max(torch.abs(num_grads['W'] - grads['W'])).item())\n",
    "print(\"Max diff for gradient of U:\", torch.max(torch.abs(num_grads['U'] - grads['U'])).item())\n",
    "print(\"Max diff for gradient of V:\", torch.max(torch.abs(num_grads['V'] - grads['V'])).item())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 10\n",
    "smooth_loss = 0\n",
    "losses = []\n",
    "hprev = RNN['h0']\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_chars = book_data[e:e+seq_length]\n",
    "    Y_chars = book_data[e+1:e+seq_length+1]\n",
    "    X_train = encode_string(X_chars)\n",
    "    Y_train = encode_string(Y_chars)\n",
    "\n",
    "    _, _, A_train, H_train, P_train, ht = forward(RNN, X_train, hprev)\n",
    "    loss = compute_loss(Y_train, P_train)\n",
    "    grads, grads_clamped = backward(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] += grads_clamped[k]**2\n",
    "        RNN[k] -= (eta/torch.sqrt(ms[k] + epsilon))*grads_clamped[k]\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev, X_train[:, 0], 200)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev, X_train[:, 0], 1000)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += seq_length\n",
    "    if e > len(book_data) - seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = RNN['h0']\n",
    "    else:\n",
    "        hprev = ht"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
