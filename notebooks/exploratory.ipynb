{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eloidieme/dev/python-projects/rnn\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107542"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_fname = \"./data/goblet_book.txt\"\n",
    "with open(book_fname, 'r') as book:\n",
    "    book_data = book.read()\n",
    "len(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = book_data.split()\n",
    "chars = [[*word] for word in word_list]\n",
    "max_len = max(len(word) for word in chars)\n",
    "for wordl in chars:\n",
    "    while len(wordl) < max_len:\n",
    "        wordl.append(' ')\n",
    "chars = np.array(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars = list(np.unique(chars))\n",
    "unique_chars.append('\\n')\n",
    "unique_chars.append('\\t')\n",
    "K = len(unique_chars)  # dimensionality of the input and output vectors\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "for idx, char in enumerate(unique_chars):\n",
    "    char_to_ind[char] = idx\n",
    "    ind_to_char[idx] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100  # dimensionality of the hidden state\n",
    "eta = 0.1  # learning rate\n",
    "seq_length = 25  # length of input sequences used during training\n",
    "epsilon = 1e-8  # for AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 0.01\n",
    "RNN = {'b': torch.zeros((m, 1), dtype=torch.double), 'c': torch.zeros((K, 1), dtype=torch.double), 'U': torch.normal(0.0, sig, (m, K), dtype=torch.double), 'W': torch.normal(0.0, sig, (m, m), dtype=torch.double), 'V': torch.normal(0.0, sig, (K, m), dtype=torch.double), 'h0': torch.zeros((m, 1), dtype=torch.double)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(char):\n",
    "    oh = [0]*K\n",
    "    oh[char_to_ind[char]] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetize_seq(rnn, h0, x0, n, T = 1):\n",
    "    t, ht, xt = 0, h0, x0\n",
    "    indexes = []\n",
    "    while t < n:\n",
    "        xt = xt.reshape((K, 1))\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b']\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c']\n",
    "        pt = F.softmax(ot/T, dim=0)\n",
    "        cp = torch.cumsum(pt, dim=0)\n",
    "        a = torch.rand(1)\n",
    "        ixs = torch.where(cp - a > 0)\n",
    "        ii = ixs[0][0].item()\n",
    "        indexes.append(ii)\n",
    "        xt = torch.zeros((K, 1), dtype=torch.double)\n",
    "        xt[ii, 0] = 1\n",
    "        t += 1\n",
    "    Y = []\n",
    "    for idx in indexes:\n",
    "        oh = [0]*K\n",
    "        oh[idx] = 1\n",
    "        Y.append(oh)\n",
    "    Y = torch.tensor(Y).t()\n",
    "    \n",
    "    s = ''\n",
    "    for i in range(Y.shape[1]):\n",
    "        idx = torch.where(Y[:, i] == 1)[0].item()\n",
    "        s += ind_to_char[idx]\n",
    "    \n",
    "    return Y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chars = book_data[:seq_length]\n",
    "Y_chars = book_data[1:seq_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HARRY POTTER AND THE GOBL'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARRY POTTER AND THE GOBLE'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_string(chars):\n",
    "    M = []\n",
    "    for i in range(len(chars)):\n",
    "        M.append(encode_char(chars[i]))\n",
    "    M = torch.tensor(M, dtype=torch.double).t()\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encode_string(X_chars)\n",
    "Y = encode_string(Y_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 25]) torch.Size([80, 25])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vh_M\tTd^_T)L2:RyMygY.kOqLz_'YG/c}w7}2Q_'Jl1PO\tgdHcDB9/(/BNDüCqVF^KfUtsUoDvn,QB/ZOKr(\t;hve2Zrx\"ümz)}nZQ.•WIn!Iü\tqRd\"b, 2O}'-J(ypCsUpskFfc13.Go!.qh y2,nTZs/\n",
      "e-V_yB()x^.oWgp9zT9YY)iI,p!•rJWJUBgR4ym7 rKL•\n"
     ]
    }
   ],
   "source": [
    "Y0, s0 = synthetize_seq(RNN, RNN['h0'], X[:,0], 200)\n",
    "print(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(rnn, X, hprev):\n",
    "    ht = hprev.clone()\n",
    "    indexes = []\n",
    "    P = torch.zeros((K, seq_length), dtype=torch.double)\n",
    "    A = torch.zeros((m, seq_length), dtype=torch.double)\n",
    "    H = torch.zeros((m, seq_length), dtype=torch.double)\n",
    "    for i in range(seq_length):\n",
    "        xt = X[:, i].reshape((K, 1))\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b']\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c']\n",
    "        pt = F.softmax(ot, dim=0)\n",
    "\n",
    "        H[:, i] = ht.squeeze()\n",
    "        P[:, i] = pt.squeeze()\n",
    "        A[:, i] = at.squeeze()\n",
    "        cp = torch.cumsum(pt, dim=0)\n",
    "        a = torch.rand(1)\n",
    "        ixs = torch.where(cp - a > 0)\n",
    "        ii = ixs[0][0].item()\n",
    "        indexes.append(ii)\n",
    "\n",
    "    Y_pred = []\n",
    "    for idx in indexes:\n",
    "        oh = [0]*K\n",
    "        oh[idx] = 1\n",
    "        Y_pred.append(oh)\n",
    "    Y_pred = torch.tensor(Y_pred, dtype=torch.double).t()\n",
    "\n",
    "    s_pred = ''\n",
    "    for i in range(Y_pred.shape[1]):\n",
    "        idx = torch.where(Y_pred[:, i] == 1)[0].item()\n",
    "        s_pred += ind_to_char[idx]\n",
    "\n",
    "    return s_pred, Y_pred, A, H, P, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f/UyoE'XbyDw} iiRYAgKFt•L\n",
      "torch.Size([80, 25])\n",
      "torch.Size([100, 25])\n",
      "torch.Size([100, 25])\n",
      "torch.Size([80, 25])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "s_pred, Y_pred, A, H, P, ht = forward(RNN, X, RNN['h0'])\n",
    "print(s_pred, Y_pred.shape, A.shape, H.shape, P.shape, ht.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, P):\n",
    "    log_probs = torch.log(P)\n",
    "    cross_entropy = -torch.sum(Y * log_probs)\n",
    "    loss = cross_entropy.item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.55973968577538\n"
     ]
    }
   ],
   "source": [
    "print(compute_loss(Y, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(rnn, X, Y, A, H, P, hprev):\n",
    "    dA = torch.zeros_like(A)\n",
    "    dH = torch.zeros_like(H)\n",
    "\n",
    "    G = -(Y - P)\n",
    "    dV = torch.matmul(G, H.t())\n",
    "    dhtau = torch.matmul(G[:, -1], rnn['V'])\n",
    "    datau = (1 - torch.pow(torch.tanh(A[:, -1]), 2)) * dhtau\n",
    "    dH[:, -1] = dhtau.squeeze()\n",
    "    dA[:, -1] = datau.squeeze()\n",
    "\n",
    "    for i in range(seq_length - 2, -1, -1):\n",
    "        dht = torch.matmul(G[:, i], rnn['V']) + torch.matmul(dA[:, i+1].reshape(1, -1), rnn['W'])\n",
    "        dat = (1 - torch.pow(torch.tanh(A[:, i]), 2)) * dht\n",
    "        dH[:, i] = dht.squeeze()\n",
    "        dA[:, i] = dat.squeeze()\n",
    "\n",
    "    Hd = torch.cat((hprev, H[:, :-1]), dim=1)\n",
    "    dW = torch.matmul(dA, Hd.t())\n",
    "    dU = torch.matmul(dA, X.t())\n",
    "    dc = G.sum(1).reshape((-1, 1))\n",
    "    db = dA.sum(1).reshape((-1, 1))\n",
    "    grads = {'U': dU, 'W': dW, 'V': dV, 'c': dc, 'b': db}\n",
    "    grads_clamped = {k: torch.clamp(v, min=-5.0, max=5.0) for (k,v) in grads.items()}\n",
    "    return grads, grads_clamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 80])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([80, 100])\n",
      "torch.Size([80, 1])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "grads, grads_clamped = backward(RNN, X, Y, A, H, P, RNN['h0'])\n",
    "print(*list(map(lambda v: grads_clamped[v].shape, grads_clamped)), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradNum(X, Y, param_name, rnn, h=1e-4):\n",
    "    \"\"\"\n",
    "    Compute the numerical gradient of the rnn's parameter specified by param_name.\n",
    "    \"\"\"\n",
    "    grad = torch.zeros_like(rnn[param_name])\n",
    "    hprev = rnn['h0']\n",
    "    n = torch.numel(rnn[param_name])\n",
    "    \n",
    "    for i in range(n):\n",
    "        old_val = rnn[param_name].view(-1)[i].item()\n",
    "        rnn[param_name].view(-1)[i] = old_val - h\n",
    "        _, _, _, _, P, _ = forward(rnn, X, hprev)\n",
    "        l1 = compute_loss(Y, P)\n",
    "        \n",
    "        rnn[param_name].view(-1)[i] = old_val + h\n",
    "        _, _, _, _, P, _ = forward(rnn, X, hprev)\n",
    "        l2 = compute_loss(Y, P)\n",
    "        \n",
    "        grad.view(-1)[i] = (l2 - l1) / (2 * h)\n",
    "        rnn[param_name].view(-1)[i] = old_val  # Reset to original value\n",
    "\n",
    "    return grad\n",
    "\n",
    "def ComputeGradsNum(X, Y, rnn, h=1e-4):\n",
    "    num_grads = {}\n",
    "    for param_name in rnn:\n",
    "        if param_name != 'h0':\n",
    "            print('Computing numerical gradient for')\n",
    "            print(f'Field name: {param_name}')\n",
    "            num_grads[param_name] = ComputeGradNum(X, Y, param_name, rnn, h)\n",
    "    return num_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grads = ComputeGradsNum(X, Y, RNN, 1e-4)\n",
    "print(\"-------- Gradient validation --------\")\n",
    "print(\"Max diff for gradient of b:\", torch.max(torch.abs(num_grads['b'] - grads['b'])).item())\n",
    "print(\"Max diff for gradient of c:\", torch.max(torch.abs(num_grads['c'] - grads['c'])).item())\n",
    "print(\"Max diff for gradient of W:\", torch.max(torch.abs(num_grads['W'] - grads['W'])).item())\n",
    "print(\"Max diff for gradient of U:\", torch.max(torch.abs(num_grads['U'] - grads['U'])).item())\n",
    "print(\"Max diff for gradient of V:\", torch.max(torch.abs(num_grads['V'] - grads['V'])).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "\t * Smooth loss: 109.55973968577538\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "k7P.IRhcejH:'P3Xv\n",
      "x17,Z•xyt;_Eda0F4fB1lsQ:o•3h,i Hx.o\"j}hwb;Vi7YA3I  4\tGEx(\tmJ_Lc( WVuEYhP4LW'K;7FQ\"jbr.O6LHkpLnUCZNz4jPrD\"yWLHeeIO\t3Kqkj2üJvüLFO}'mk,iV(d3oesV.kH.'DjDInx(C•KHtTeHdCrB PdjhN2eü;OjY)•gx\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 1000\n",
      "\t * Smooth loss: 87.20606686603843\n",
      "Step: 2000\n",
      "\t * Smooth loss: 72.25349064265023\n",
      "Step: 3000\n",
      "\t * Smooth loss: 64.76845092083013\n",
      "Step: 4000\n",
      "\t * Smooth loss: 61.603030376168796\n",
      "Step: 5000\n",
      "\t * Smooth loss: 59.82363185059591\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "nn of thavey the ghe hith he wore thind wourg the ghe fing mad,\"\n",
      "\"I the we the sen sus as thing flald Here to as the boung on Mis and und ang of ank out the the farttant hing are, herore fot wall nens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 6000\n",
      "\t * Smooth loss: 59.12932851298288\n",
      "Step: 7000\n",
      "\t * Smooth loss: 59.05407847125436\n",
      "Step: 8000\n",
      "\t * Smooth loss: 56.846990598528286\n",
      "Step: 9000\n",
      "\t * Smooth loss: 56.34360439007397\n",
      "Step: 10000\n",
      "\t * Smooth loss: 56.28877732626113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "and and an tha lige and to tand a fas the the thane Herid the surmero gert to the ho kek the the thit soor ve cough, wad the cong bong the singed the thew of the waury boucinting at stane ve mas sit t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 11000\n",
      "\t * Smooth loss: 56.71109626493931\n",
      "Step: 12000\n",
      "\t * Smooth loss: 55.979214618080015\n",
      "Step: 13000\n",
      "\t * Smooth loss: 55.524660380500166\n",
      "Step: 14000\n",
      "\t * Smooth loss: 54.83726142796013\n",
      "Step: 15000\n",
      "\t * Smooth loss: 54.65168339831252\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " the there exser stades ougeele boud the sen, said rof the the gerill the her hall lases the that feres of he hall at gerting the the had Marn lound hacest and Hed are of, the ares sainbe the the ofle\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 16000\n",
      "\t * Smooth loss: 54.06971463773648\n",
      "Step: 17000\n",
      "\t * Smooth loss: 53.96180692537709\n",
      "Step: 18000\n",
      "\t * Smooth loss: 53.686256783435205\n",
      "Step: 19000\n",
      "\t * Smooth loss: 53.24971349932287\n",
      "Step: 20000\n",
      "\t * Smooth loss: 52.67439251555837\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "ed wand the youghe fou are he the f the flackor to and lizare mut the he dour was whe sto the sous frair.  Horry seres he goud the as of that woud and the aily oum the sipted his an's at bought the sh\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 21000\n",
      "\t * Smooth loss: 52.38787225501461\n",
      "Step: 22000\n",
      "\t * Smooth loss: 51.757139747112184\n",
      "Step: 23000\n",
      "\t * Smooth loss: 52.054665672932785\n",
      "Step: 24000\n",
      "\t * Smooth loss: 52.037241226060104\n",
      "Step: 25000\n",
      "\t * Smooth loss: 52.08039214957863\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " and of had the with her in that tery front sarry in the at apperteronting't he sas on was Harry.  Harry fut was wing the sair wald the said sto seim, a was wath hart he the dement the was Harr has a \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 26000\n",
      "\t * Smooth loss: 51.94463954691736\n",
      "Step: 27000\n",
      "\t * Smooth loss: 51.99220472234716\n",
      "Step: 28000\n",
      "\t * Smooth loss: 52.057497958269074\n",
      "Step: 29000\n",
      "\t * Smooth loss: 51.72043113106551\n",
      "Step: 30000\n",
      "\t * Smooth loss: 51.19106436745977\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "es anter and enst and tho mow was, plisce gots his that and the sound then ale on a the sean was wand when sally fors the she seres sar bing at his surne at and lonk of still opsing the seabe the sher\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 31000\n",
      "\t * Smooth loss: 51.39439264704984\n",
      "Step: 32000\n",
      "\t * Smooth loss: 51.084280302752994\n",
      "Step: 33000\n",
      "\t * Smooth loss: 51.158820640757654\n",
      "Step: 34000\n",
      "\t * Smooth loss: 50.463516533260425\n",
      "Step: 35000\n",
      "\t * Smooth loss: 50.219673927039594\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "t.  Harry had fark and on wis a have a had meald a caye her, was aming the had the of pad at and the gind.  \"I pofe, in ast of the his his gound the repen't weres stand nou the you he carving at caw t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 36000\n",
      "\t * Smooth loss: 50.40561006598643\n",
      "Step: 37000\n",
      "\t * Smooth loss: 50.57339564583223\n",
      "Step: 38000\n",
      "\t * Smooth loss: 50.35787974062157\n",
      "Step: 39000\n",
      "\t * Smooth loss: 49.97494793442444\n",
      "Step: 40000\n",
      "\t * Smooth loss: 50.232628530433736\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "heme would withe parg and the wiss the reated in the sid berile fild nort suce beare, she his the drood the was the chat heare as this beed and the reatling the wore frot he her sulled onet the be the\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 41000\n",
      "\t * Smooth loss: 49.280997180425146\n",
      "Step: 42000\n",
      "\t * Smooth loss: 49.331650150908814\n",
      "Step: 43000\n",
      "\t * Smooth loss: 49.69375767270342\n",
      "Step: 44000\n",
      "\t * Smooth loss: 49.624303058669206\n",
      "Step: 45000\n",
      "\t * Smooth loss: 50.69378626351803\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "  Mould.\"\n",
      "\"I beant sore bour,\" said fofle whot so mofl Frow sill is.\n",
      "\t\"I he were sour sard to same to rain so cares to the were and as in said pure the aft and slach stare and the lould he pare ha dow\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 46000\n",
      "\t * Smooth loss: 50.83087185465079\n",
      "Step: 47000\n",
      "\t * Smooth loss: 50.68769239136828\n",
      "Step: 48000\n",
      "\t * Smooth loss: 50.22450434617694\n",
      "Step: 49000\n",
      "\t * Smooth loss: 50.58071557342673\n",
      "Step: 50000\n",
      "\t * Smooth loss: 51.04152620773807\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " pemald him of tand ream on the stered froldang.\"\n",
      "\"I wance to the geild surall wering the gart at and stirted the Mas ass said hand ak Pery of dinge being the in figh of and there areed and hand of an\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 51000\n",
      "\t * Smooth loss: 52.191792354476945\n",
      "Step: 52000\n",
      "\t * Smooth loss: 50.48795393855486\n",
      "Step: 53000\n",
      "\t * Smooth loss: 49.37854211984656\n",
      "Step: 54000\n",
      "\t * Smooth loss: 49.83827824190858\n",
      "Step: 55000\n",
      "\t * Smooth loss: 51.2691665555036\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " Gair the fore the stould of ever sour a the soring the praixp Mrsare was and her at and he wing the eef freak the spach and Ron of the caled at at the the - but the wist one the had on the meang to s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 56000\n",
      "\t * Smooth loss: 51.260879443254986\n",
      "Step: 57000\n",
      "\t * Smooth loss: 50.836449570615464\n",
      "Step: 58000\n",
      "\t * Smooth loss: 49.86864278902199\n",
      "Step: 59000\n",
      "\t * Smooth loss: 50.723338040539254\n",
      "Step: 60000\n",
      "\t * Smooth loss: 49.88878346317108\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "red a acch be down the came the'd apfall the surs wiss said Greans backing waid.\"\n",
      "\"I was sumpey.  Hurry, see geatse, aid He Dret what and they to geage, untif on hand the mabd.  \"I startst she staill.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 61000\n",
      "\t * Smooth loss: 50.12918918467184\n",
      "Step: 62000\n",
      "\t * Smooth loss: 49.58608112646294\n",
      "Step: 63000\n",
      "\t * Smooth loss: 49.83576598254248\n",
      "Step: 64000\n",
      "\t * Smooth loss: 49.31937704642066\n",
      "Step: 65000\n",
      "\t * Smooth loss: 49.11744223113446\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "said the waght as to beter theme to ares had then gomed to to that the hear doung soing to hould the cood mork of to dask keone sede had that a towaly in got the walf him quirtalf to on the torly obli\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 66000\n",
      "\t * Smooth loss: 48.417302120317494\n",
      "Step: 67000\n",
      "\t * Smooth loss: 48.8717873136844\n",
      "Step: 68000\n",
      "\t * Smooth loss: 49.18380701214814\n",
      "Step: 69000\n",
      "\t * Smooth loss: 49.31555866514105\n",
      "Step: 70000\n",
      "\t * Smooth loss: 49.30863466184492\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " her the dert are wifly a the to the suid, lould of fere roar beatwed one he has on on in and the very.  \"I menter is and pack the was keted on't heack ally ther the dook.\"\n",
      "\"Oh the he pat and he his e\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 71000\n",
      "\t * Smooth loss: 49.64848178962342\n",
      "Step: 72000\n",
      "\t * Smooth loss: 49.52645932339175\n",
      "Step: 73000\n",
      "\t * Smooth loss: 49.36529284481798\n",
      "Step: 74000\n",
      "\t * Smooth loss: 49.05056106856443\n",
      "Step: 75000\n",
      "\t * Smooth loss: 48.597614598303565\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "as were the starly fardy the Cupsly mick and had comate, he on wime he her and tarut came the touke trobats wost the beathough the crousary stowted out wat had a and semass in the and and the non, whe\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 76000\n",
      "\t * Smooth loss: 48.56004269711798\n",
      "Step: 77000\n",
      "\t * Smooth loss: 48.57823422531397\n",
      "Step: 78000\n",
      "\t * Smooth loss: 48.21869334297316\n",
      "Step: 79000\n",
      "\t * Smooth loss: 47.55757258055148\n",
      "Step: 80000\n",
      "\t * Smooth loss: 47.88595279220299\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "ger here going the of they porntisceond was in tat had sto was there reare to wis, I could the and to seidered and the for of the dout be had the core the geace now, whomast Harry and he the souch wan\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 81000\n",
      "\t * Smooth loss: 47.5550628540892\n",
      "Step: 82000\n",
      "\t * Smooth loss: 48.05428103345289\n",
      "Step: 83000\n",
      "\t * Smooth loss: 47.71808487345795\n",
      "Step: 84000\n",
      "\t * Smooth loss: 48.679592505557515\n",
      "Step: 85000\n",
      "\t * Smooth loss: 47.33544898916333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "ng the was of at sive the neall be waid to was the drow the hid his the wand she shought was see sould get were a at look him I man over his ponted ever for the of the be and the shing he med fore the\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 86000\n",
      "\t * Smooth loss: 47.509926860796256\n",
      "Step: 87000\n",
      "\t * Smooth loss: 47.75375202006795\n",
      "Step: 88000\n",
      "\t * Smooth loss: 47.60329057483641\n",
      "Step: 89000\n",
      "\t * Smooth loss: 48.35306375185308\n",
      "Step: 90000\n",
      "\t * Smooth loss: 48.651321696494406\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "nd was him of the goor in tand a with it his soul the to mores of a hagbly was the dimare, bay was was a what the farting a he werced that his be do her bet and she to no the wis and Filted it the for\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 91000\n",
      "\t * Smooth loss: 49.27291075037166\n",
      "Step: 92000\n",
      "\t * Smooth loss: 48.70254230428705\n",
      "Step: 93000\n",
      "\t * Smooth loss: 48.87663689082164\n",
      "Step: 94000\n",
      "\t * Smooth loss: 49.38697956099372\n",
      "Step: 95000\n",
      "\t * Smooth loss: 50.40452503745388\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " NETEAd Mr. Weant the said arded that a lall, shan of the the would the dirned firred dote he dick ot a meally listeld, seale of the slaring the mast and stainte the looking ag the of the lind than fa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 96000\n",
      "\t * Smooth loss: 49.70155428664636\n",
      "Step: 97000\n",
      "\t * Smooth loss: 48.16475812940179\n",
      "Step: 98000\n",
      "\t * Smooth loss: 48.38583920188033\n",
      "Step: 99000\n",
      "\t * Smooth loss: 49.091896192259625\n",
      "Step: 100000\n",
      "\t * Smooth loss: 49.61920909250751\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " said Ron iven there and the said a him any Durme over out he reall on it a her beet of and Harry of intere ext wis and wand Mr. Weased trown mean on to deare the wis the speened was and a ast the sai\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Long synthetized sequence: \n",
      " on the stied Weas encate the Harry sharle sithening at to the stay the stalry, and shat the stould tere has this they sole to the mares.  \"Che sill her.  \"What of the stant a lided fermios.  hat and of the to sligel to what of of look in the made and the slid up the the dich the surned and was seades ake the rees hare doo the aid Harry, befored though all and the teats came last the been the seave tis with the reled the silling the staire and the sonting the stat a lood, Ron's starring to and the said lered was wis to stard it in the Dacked hor him the sould the surted that and to groone he manis and the chat and the moat and of the seen the stars a fagof the the made the Durre?\" said Ron's said Harry wand ageing unt who mar wome the spade with the said a care to the Ground and ey to stich saw right the sees and in chat a ferte are the hengolled foing Harry fore the suare the partion he said a wave starker fam the grouter the shan they a was to thoul to vely the a sius are with and th\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 101000\n",
      "\t * Smooth loss: 49.652924161249416\n",
      "Step: 102000\n",
      "\t * Smooth loss: 48.62961685287037\n",
      "Step: 103000\n",
      "\t * Smooth loss: 49.43244601067621\n",
      "Step: 104000\n",
      "\t * Smooth loss: 48.68689013698349\n",
      "Step: 105000\n",
      "\t * Smooth loss: 48.325106515657914\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " his laght chemed of the was in the Durmen and the meace dight the shaning as it the tooll and Harry her were you and trip and the out and bould to of the who the bad the Harry sulled diome at the pai\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 106000\n",
      "\t * Smooth loss: 48.06507627010856\n",
      "Step: 107000\n",
      "\t * Smooth loss: 48.36530136512674\n",
      "Step: 108000\n",
      "\t * Smooth loss: 48.05150162936563\n",
      "Step: 109000\n",
      "\t * Smooth loss: 47.800880932705866\n",
      "Step: 110000\n",
      "\t * Smooth loss: 47.49112804812367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "n hear have him.  Harre.  Harriding ey a stilled the cere he sotand though splow, whan he clurwar was him the row a but glade the lood as he wamritger every juds acking and were the the digging the th\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 111000\n",
      "\t * Smooth loss: 47.663530365212495\n",
      "Step: 112000\n",
      "\t * Smooth loss: 47.87797173270219\n",
      "Step: 113000\n",
      "\t * Smooth loss: 48.01608421072201\n",
      "Step: 114000\n",
      "\t * Smooth loss: 48.19663491507298\n",
      "Step: 115000\n",
      "\t * Smooth loss: 48.588085972669525\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " his a back in the was the partly and the tlew and the shane to gofasery and were and reatione cachars and Harry sables sert.  He his has awly shelled tis seare was than had and the looding the betor \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 116000\n",
      "\t * Smooth loss: 47.77432731021155\n",
      "Step: 117000\n",
      "\t * Smooth loss: 48.49892485146336\n",
      "Step: 118000\n",
      "\t * Smooth loss: 48.24504364097509\n",
      "Step: 119000\n",
      "\t * Smooth loss: 47.20488586484801\n",
      "Step: 120000\n",
      "\t * Smooth loss: 47.6782878012187\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " ound had and the sarts of surmione a have to aron wather smile with a there tow the be that on the ance theak of the engrens the sperssed found of booke some the to has whow was curse have him Harry \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 121000\n",
      "\t * Smooth loss: 47.326054578515524\n",
      "Step: 122000\n",
      "\t * Smooth loss: 47.664124771483245\n",
      "Step: 123000\n",
      "\t * Smooth loss: 46.2949563683659\n",
      "Step: 124000\n",
      "\t * Smooth loss: 46.40483281319672\n",
      "Step: 125000\n",
      "\t * Smooth loss: 46.72242210754632\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "int of the to sirtave suals?\" said the de sills.  \"You gay in he said beated forn the said was in the not Croupterout the Sarmen were in the of to was seid, buttere the subles sull sumbe to awing of t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 126000\n",
      "\t * Smooth loss: 46.94160604927771\n",
      "Step: 127000\n",
      "\t * Smooth loss: 46.9483320012367\n",
      "Step: 128000\n",
      "\t * Smooth loss: 47.729054403339816\n",
      "Step: 129000\n",
      "\t * Smooth loss: 46.81447575329605\n",
      "Step: 130000\n",
      "\t * Smooth loss: 46.78103349217113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "owth who cof atery.\n",
      "\"Ae the saver the sered he dinding and was in the going to her linded with hin bes fres of the shem wiman'e looked and bay the mamione to the said a as in to and mocend of the fin \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 131000\n",
      "\t * Smooth loss: 46.19944659020136\n",
      "Step: 132000\n",
      "\t * Smooth loss: 46.47259280488563\n",
      "Step: 133000\n",
      "\t * Smooth loss: 47.08703162788957\n",
      "Step: 134000\n",
      "\t * Smooth loss: 47.74390031001503\n",
      "Step: 135000\n",
      "\t * Smooth loss: 48.26990393290255\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "s and to to the sough shis the pell eass was he right fay, with sip would - at the poreden he dorm the her his the goon the said sto of for stantion his of had booked the sull so fore Harry as sat had\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 136000\n",
      "\t * Smooth loss: 47.878604934787646\n",
      "Step: 137000\n",
      "\t * Smooth loss: 47.85266995310106\n",
      "Step: 138000\n",
      "\t * Smooth loss: 47.79388611097666\n",
      "Step: 139000\n",
      "\t * Smooth loss: 48.56283867115162\n",
      "Step: 140000\n",
      "\t * Smooth loss: 49.56794885585731\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "e - fere the could were the re her the mear him the wist the silge and Harry night there stale a the like, arered fully aron werry the goth, It!\"  she siught of the plasss queterow was be theysled a s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 141000\n",
      "\t * Smooth loss: 47.52351969101869\n",
      "Step: 142000\n",
      "\t * Smooth loss: 47.236454709522505\n",
      "Step: 143000\n",
      "\t * Smooth loss: 47.767947150023396\n",
      "Step: 144000\n",
      "\t * Smooth loss: 48.79679532490333\n",
      "Step: 145000\n",
      "\t * Smooth loss: 48.78071528736917\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "the forming to the was the shan the what and the chem had have all dented.  I here, \"Mr..\n",
      "\"You him the lat stome the somance all the surte of the was op they wart expear and was at sheal sough the be \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 146000\n",
      "\t * Smooth loss: 47.988286977064305\n",
      "Step: 147000\n",
      "\t * Smooth loss: 48.44548601599847\n",
      "Step: 148000\n",
      "\t * Smooth loss: 47.858491660814295\n",
      "Step: 149000\n",
      "\t * Smooth loss: 47.50022352970735\n",
      "Step: 150000\n",
      "\t * Smooth loss: 47.48707329575837\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "t the starped leing they wis of had on on the been stail the Dumbleron the taght eaned he was streen trened the macked a are on to camestore than en shen the wisted around, For had ow, when her you' d\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 151000\n",
      "\t * Smooth loss: 47.32029929837322\n",
      "Step: 152000\n",
      "\t * Smooth loss: 47.4416817927297\n",
      "Step: 153000\n",
      "\t * Smooth loss: 46.98418784296994\n",
      "Step: 154000\n",
      "\t * Smooth loss: 46.677071212915344\n",
      "Step: 155000\n",
      "\t * Smooth loss: 46.4552899619969\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "packed in to sill a furging the said the did when sealed awore were a could him, and the the drich the and the wall the minting and to his mould the wist's have other, for ithen the ternonse theo was \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 156000\n",
      "\t * Smooth loss: 46.68748071811431\n",
      "Step: 157000\n",
      "\t * Smooth loss: 46.72982783459975\n",
      "Step: 158000\n",
      "\t * Smooth loss: 47.264464157863564\n",
      "Step: 159000\n",
      "\t * Smooth loss: 47.24059152909003\n",
      "Step: 160000\n",
      "\t * Smooth loss: 47.23454128082684\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "the looked of home, she viss at the showar, a dering and they it the been of the unted in the say in they to all to could ne to his not was a backing a could and the sparing the was beturing to a tell\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 161000\n",
      "\t * Smooth loss: 47.50695221728388\n",
      "Step: 162000\n",
      "\t * Smooth loss: 47.19550416925684\n",
      "Step: 163000\n",
      "\t * Smooth loss: 47.00048060050765\n",
      "Step: 164000\n",
      "\t * Smooth loss: 47.26023690203446\n",
      "Step: 165000\n",
      "\t * Smooth loss: 46.71851542224349\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "haiked courbe a fernicaret he up the out anly it?\"  she she tay dowt his at to doon him and and of in the night ward a swat nower, the was tare back and on beatery, him, at his and and his was bess hi\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 166000\n",
      "\t * Smooth loss: 47.0035676347059\n",
      "Step: 167000\n",
      "\t * Smooth loss: 45.733348824650136\n",
      "Step: 168000\n",
      "\t * Smooth loss: 45.95744041726033\n",
      "Step: 169000\n",
      "\t * Smooth loss: 46.026014807183046\n",
      "Step: 170000\n",
      "\t * Smooth loss: 46.460073917652046\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "to frouten with houdd Bast a hear not cerse tas her silled sor hus wromen the fail his you but the sumble, and a was looked and exchalr.\"\n",
      "\"You flest the head about the lace said and the was semall all\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 171000\n",
      "\t * Smooth loss: 46.4617671433669\n",
      "Step: 172000\n",
      "\t * Smooth loss: 46.57856723367066\n",
      "Step: 173000\n",
      "\t * Smooth loss: 46.43742419849905\n",
      "Step: 174000\n",
      "\t * Smooth loss: 45.641450605282145\n",
      "Step: 175000\n",
      "\t * Smooth loss: 45.456923054649806\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "t the bech and you one he roupperound the somach the plowned of the of faind him out the he Sarry wour and mand beater sheake rear were to the said moricss the sow the mase of the ferther homen of the\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 176000\n",
      "\t * Smooth loss: 45.770557682767624\n",
      "Step: 177000\n",
      "\t * Smooth loss: 45.82275594814578\n",
      "Step: 178000\n",
      "\t * Smooth loss: 47.411963891753565\n",
      "Step: 179000\n",
      "\t * Smooth loss: 47.800528195167075\n",
      "Step: 180000\n",
      "\t * Smooth loss: 47.445181839843094\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "o to the serese the sampe the she fore of he was lect.  Harry the reall out - Surresed and said Mr.\n",
      "But Harry, and Harry's - be the conder he said nold aring other he said the of the braned the the su\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 181000\n",
      "\t * Smooth loss: 47.14274861223405\n",
      "Step: 182000\n",
      "\t * Smooth loss: 47.20546677903388\n",
      "Step: 183000\n",
      "\t * Smooth loss: 47.964073954648356\n",
      "Step: 184000\n",
      "\t * Smooth loss: 49.23753849268816\n",
      "Step: 185000\n",
      "\t * Smooth loss: 47.20210365840868\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "nce wantion the didney grow who stand on ha five a could thoughted and wat on the she carche was gain.\n",
      "\"Goung the could we looking the seath and the stare and was nens the lecked wadone the carchers t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 186000\n",
      "\t * Smooth loss: 46.331664082969816\n",
      "Step: 187000\n",
      "\t * Smooth loss: 46.716715585992716\n",
      "Step: 188000\n",
      "\t * Smooth loss: 48.42873217099818\n",
      "Step: 189000\n",
      "\t * Smooth loss: 48.37814142215865\n",
      "Step: 190000\n",
      "\t * Smooth loss: 47.70065659994938\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " a be to the stroul the suld madion her still the siver kien the aid in of thoul him as this not the bistered as pernted and they he wizard to the Mooked mookn.  The was aed said and a and to cerrount\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 191000\n",
      "\t * Smooth loss: 46.95344280114979\n",
      "Step: 192000\n",
      "\t * Smooth loss: 47.564080700237696\n",
      "Step: 193000\n",
      "\t * Smooth loss: 46.98075792597096\n",
      "Step: 194000\n",
      "\t * Smooth loss: 47.01025759225839\n",
      "Step: 195000\n",
      "\t * Smooth loss: 46.66035141577584\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "e dreal a sto he for the starding on the trown't it the pparted through thought and her was the ppetting tarred.  \"I had bettor both the coffor thoor with in the had, the has were beins not the chave \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 196000\n",
      "\t * Smooth loss: 46.990513009286154\n",
      "Step: 197000\n",
      "\t * Smooth loss: 46.58283906788806\n",
      "Step: 198000\n",
      "\t * Smooth loss: 46.24181833069362\n",
      "Step: 199000\n",
      "\t * Smooth loss: 45.64018889816322\n",
      "Step: 200000\n",
      "\t * Smooth loss: 46.211422214419535\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " the lated the said in every, and looked stirted to the ealled the would had hand a peret dins of staman a sped thelk the sich to stard the sure said to he his hold out them said the pacing and had bo\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Long synthetized sequence: \n",
      "'s it get Harry aid Harry, and Harry a mastene I deen sich her scour in stares still have the had said a want, wonder seate a the bearing shaigh, the witt' me could the tarry - wom, and was had bark to the fromped on Sthen the fill who to were wipped an get all cond - wo bouts the she sar carous on of the sont wore frid the pay, the sir and Harry lord bank you his slid and the freen the mowred in the erbe head foing of comes and him lat sour was sat she stould do the part to wand reather stare falled the spars, and he her, hear of he he fory scan said be a stot and wort said the surte the toul fress fratt.  \"Harry, the bested houghing with hang to withed ally were the and soing and Harry was wreasle door, was on you could and head said dintled some to de the defore's werring me the oup on at the of last there was musting and the with an the tould couth in her heme the gotione the lage the carned the saided at the dive at of conder, of his the and real were the way starn was sing to to \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 201000\n",
      "\t * Smooth loss: 46.4597799332926\n",
      "Step: 202000\n",
      "\t * Smooth loss: 46.64681687147101\n",
      "Step: 203000\n",
      "\t * Smooth loss: 46.83188728509605\n",
      "Step: 204000\n",
      "\t * Smooth loss: 46.77290355167848\n",
      "Step: 205000\n",
      "\t * Smooth loss: 46.9141368950577\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      ", she was was to said theme a Parming to a plobled beent the been the would his was in that was said to were to with chat the like at the them like cothered his bet the louge the firsged and prow of h\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 206000\n",
      "\t * Smooth loss: 46.57195243026042\n",
      "Step: 207000\n",
      "\t * Smooth loss: 46.493731415905344\n",
      "Step: 208000\n",
      "\t * Smooth loss: 46.340224384824054\n",
      "Step: 209000\n",
      "\t * Smooth loss: 46.14841422355023\n",
      "Step: 210000\n",
      "\t * Smooth loss: 46.303378726323324\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "red of the was with the her the with the the to caring to keent of the freace bittor the gaise be to conted him a the mare the sone of the bet selled in che't in, and he said pattered from they he'd t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 211000\n",
      "\t * Smooth loss: 45.33778995476394\n",
      "Step: 212000\n",
      "\t * Smooth loss: 45.044987236787506\n",
      "Step: 213000\n",
      "\t * Smooth loss: 45.42489498850368\n",
      "Step: 214000\n",
      "\t * Smooth loss: 45.08206129463075\n",
      "Step: 215000\n",
      "\t * Smooth loss: 45.637647319264175\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " a he loldem, I the start he was a me and the theman fers the had and steme a the scaus the distred the was the surs he saded he chem of the sopter.  The find sould and Proung the were for the surn an\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 216000\n",
      "\t * Smooth loss: 45.47840180001356\n",
      "Step: 217000\n",
      "\t * Smooth loss: 46.30994186038274\n",
      "Step: 218000\n",
      "\t * Smooth loss: 44.96860085466191\n",
      "Step: 219000\n",
      "\t * Smooth loss: 45.27710309188719\n",
      "Step: 220000\n",
      "\t * Smooth loss: 45.31986691144235\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "ced the ond and to when to he sair you at that and he surte the to been the said the from to had them, his doon the realled on the stare stour his son to was to surtened sut inter mase wall his a were\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 221000\n",
      "\t * Smooth loss: 45.29611707428794\n",
      "Step: 222000\n",
      "\t * Smooth loss: 46.35123342461876\n",
      "Step: 223000\n",
      "\t * Smooth loss: 46.35710126362927\n",
      "Step: 224000\n",
      "\t * Smooth loss: 47.11062513313421\n",
      "Step: 225000\n",
      "\t * Smooth loss: 46.30350186383528\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "id it cond you will was semars, whing the been thee say op the Dim of his he hers.  The not of him to firen's work they be and the subbed to conting on paght the the said the arry entort selately for \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 226000\n",
      "\t * Smooth loss: 46.7293031151139\n",
      "Step: 227000\n",
      "\t * Smooth loss: 47.15623066315163\n",
      "Step: 228000\n",
      "\t * Smooth loss: 48.406271838234304\n",
      "Step: 229000\n",
      "\t * Smooth loss: 47.29523018254575\n",
      "Step: 230000\n",
      "\t * Smooth loss: 45.56549509377682\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "and rears all the stares, excrione.  Harry a stont of the sere. . . . he said Mr. Weasley's a word a cont the rink of the toupter a mestion at wrow the wioked it.  \"Ye and stersully that, sianing at w\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 231000\n",
      "\t * Smooth loss: 46.0160492039183\n",
      "Step: 232000\n",
      "\t * Smooth loss: 46.891831310129845\n",
      "Step: 233000\n",
      "\t * Smooth loss: 47.454892794315974\n",
      "Step: 234000\n",
      "\t * Smooth loss: 47.28092506989909\n",
      "Step: 235000\n",
      "\t * Smooth loss: 46.57663324463185\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "to the dindedion, and the atertar serarter the sald on to for they whith, and Harry sprowar treane the had was said to it of the lut lead with they cour, and the sair looke see hersard the just had th\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 236000\n",
      "\t * Smooth loss: 47.27434757223547\n",
      "Step: 237000\n",
      "\t * Smooth loss: 46.29548027630019\n",
      "Step: 238000\n",
      "\t * Smooth loss: 46.22929433753709\n",
      "Step: 239000\n",
      "\t * Smooth loss: 45.73573416878499\n",
      "Step: 240000\n",
      "\t * Smooth loss: 46.35694774384092\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " like the was bet jolloone on tare was the delack the wall tour, skofe, but the said had soor the tarmed he say fark on a who have had could and the said.  He said Harry pack no had got ablack.  Ah sp\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 241000\n",
      "\t * Smooth loss: 45.92624615396808\n",
      "Step: 242000\n",
      "\t * Smooth loss: 45.76429123369186\n",
      "Step: 243000\n",
      "\t * Smooth loss: 45.519875900999686\n",
      "Step: 244000\n",
      "\t * Smooth loss: 45.94886852271563\n",
      "Step: 245000\n",
      "\t * Smooth loss: 45.905032754188674\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "e the from sto at one ther were to and inver the boware tert his he cans onsed a sking to said to was the the bett fare of hough to and with of the didere and was scare and it coor a purply was a men,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 246000\n",
      "\t * Smooth loss: 45.887884105402584\n",
      "Step: 247000\n",
      "\t * Smooth loss: 46.06342488097517\n",
      "Step: 248000\n",
      "\t * Smooth loss: 46.988098168829055\n",
      "Step: 249000\n",
      "\t * Smooth loss: 46.10191542199574\n",
      "Step: 250000\n",
      "\t * Smooth loss: 46.68691772874136\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "or the pell to said and though.. . . . . . . I've his said.\n",
      "\"He chat ongor the care the spould the Find at claded at and to hooked to man the was more the sarred and be to wall more of the staid, star\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 251000\n",
      "\t * Smooth loss: 46.4142827560495\n",
      "Step: 252000\n",
      "\t * Smooth loss: 45.539344012257914\n",
      "Step: 253000\n",
      "\t * Smooth loss: 46.00495214145345\n",
      "Step: 254000\n",
      "\t * Smooth loss: 45.65761638851889\n",
      "Step: 255000\n",
      "\t * Smooth loss: 45.632949254462865\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "mblermalled and the some the conter to nogped and his becther hed for in the startly at the bour you ast her what you his midon to cour givister staring he he the off a so of he will of that wagped th\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 256000\n",
      "\t * Smooth loss: 44.16818087585284\n",
      "Step: 257000\n",
      "\t * Smooth loss: 44.450600913334874\n",
      "Step: 258000\n",
      "\t * Smooth loss: 44.55412334939381\n",
      "Step: 259000\n",
      "\t * Smooth loss: 45.05539697761704\n",
      "Step: 260000\n",
      "\t * Smooth loss: 45.0573949231698\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "the spising if the shen fure the he said, was the say and the sing the sling the was the said timing the stomettien woll she roundy.  The slide the staght and that be ins at at head awar.\n",
      "\"Wear he des\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 261000\n",
      "\t * Smooth loss: 45.9530137208188\n",
      "Step: 262000\n",
      "\t * Smooth loss: 44.77908924461207\n",
      "Step: 263000\n",
      "\t * Smooth loss: 44.818285243329946\n",
      "Step: 264000\n",
      "\t * Smooth loss: 44.44509170342604\n",
      "Step: 265000\n",
      "\t * Smooth loss: 44.46303987470442\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "t hor in and for the said don at the chate the his brath the he dissed had wer his at the sill the the tolor the who whate him eyes to was allen the tonting the shail to dore they, he was the bather t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 266000\n",
      "\t * Smooth loss: 45.55164619162331\n",
      "Step: 267000\n",
      "\t * Smooth loss: 46.09635272880212\n",
      "Step: 268000\n",
      "\t * Smooth loss: 46.47135852273529\n",
      "Step: 269000\n",
      "\t * Smooth loss: 45.99385523593647\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m X_train \u001b[38;5;241m=\u001b[39m encode_string(X_chars)\n\u001b[1;32m     18\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m encode_string(Y_chars)\n\u001b[0;32m---> 20\u001b[0m _, _, A_train, H_train, P_train, ht \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhprev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(Y_train, P_train)\n\u001b[1;32m     22\u001b[0m grads, grads_clamped \u001b[38;5;241m=\u001b[39m backward(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(rnn, X, hprev)\u001b[0m\n\u001b[1;32m     18\u001b[0m     a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m     ixs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cp \u001b[38;5;241m-\u001b[39m a \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     ii \u001b[38;5;241m=\u001b[39m \u001b[43mixs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     indexes\u001b[38;5;241m.\u001b[39mappend(ii)\n\u001b[1;32m     23\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 10\n",
    "smooth_loss = 0\n",
    "losses = []\n",
    "hprev = RNN['h0']\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_chars = book_data[e:e+seq_length]\n",
    "    Y_chars = book_data[e+1:e+seq_length+1]\n",
    "    X_train = encode_string(X_chars)\n",
    "    Y_train = encode_string(Y_chars)\n",
    "\n",
    "    _, _, A_train, H_train, P_train, ht = forward(RNN, X_train, hprev)\n",
    "    loss = compute_loss(Y_train, P_train)\n",
    "    grads, grads_clamped = backward(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] += grads_clamped[k]**2\n",
    "        RNN[k] -= (eta/torch.sqrt(ms[k] + epsilon))*grads_clamped[k]\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev, X_train[:, 0], 200, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev, X_train[:, 0], 1000, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += seq_length\n",
    "    if e > len(book_data) - seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = RNN['h0']\n",
    "    else:\n",
    "        hprev = ht\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.grid(True)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Smooth loss')\n",
    "plt.title(f'Training with AdaGrad - eta: {eta} - seq_length: {seq_length} - m: {m} - n_epochs: {n_epochs}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnn_adam.pickle', 'rb') as handle:\n",
    "    test_rnn = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_t, s_t = synthetize_seq(test_rnn, test_rnn['h0'], X[:,0], 1000, 0.1)\n",
    "print(\"H\" + s_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus points\n",
    "#### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 10\n",
    "smooth_loss = 0\n",
    "losses = []\n",
    "hprev = RNN['h0']\n",
    "\n",
    "beta_1, beta_2, epsilon = 0.9, 0.999, 1e-8\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "vb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "vc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "vU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "vV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "vW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "vs = {'b': vb, 'c': vc, 'U': vU, 'V': vV, 'W': vW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_chars = book_data[e:e+seq_length]\n",
    "    Y_chars = book_data[e+1:e+seq_length+1]\n",
    "    X_train = encode_string(X_chars)\n",
    "    Y_train = encode_string(Y_chars)\n",
    "\n",
    "    _, _, A_train, H_train, P_train, ht = forward(RNN, X_train, hprev)\n",
    "    loss = compute_loss(Y_train, P_train)\n",
    "    grads, grads_clamped = backward(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] = beta_1*ms[k] + (1 - beta_1)*grads_clamped[k]\n",
    "        vs[k] = beta_2*vs[k] + (1 - beta_2)*(grads_clamped[k]**2)\n",
    "        m_hat = ms[k]/(1 - beta_1**(step+1))\n",
    "        v_hat = vs[k]/(1 - beta_2**(step+1))\n",
    "        RNN[k] -= (eta/torch.sqrt(v_hat + epsilon))*m_hat\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev, X_train[:, 0], 200)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev, X_train[:, 0], 1000)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += seq_length\n",
    "    if e > len(book_data) - seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = RNN['h0']\n",
    "    else:\n",
    "        hprev = ht\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, epoch = 0, 0\n",
    "n_epochs = 10\n",
    "smooth_loss = 0\n",
    "losses = []\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    L = random.randint(120, 150)\n",
    "    print(f\"\\t * No. chunks: {L}\")\n",
    "    chunks = []\n",
    "    chunk_size = len(book_data)//L\n",
    "    print(f\"\\t * Chunk size: {chunk_size}\")\n",
    "    for i in range(L - 1):\n",
    "        chunks.append(book_data[i:i+chunk_size])\n",
    "    chunks.append(book_data[-chunk_size:])\n",
    "    random.shuffle(chunks)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"-> Reached chunk {idx+1}\")\n",
    "        e = 0\n",
    "        hprev = RNN['h0']\n",
    "        while e < (len(chunk) - seq_length):\n",
    "            X_chars = chunk[e:e+seq_length]\n",
    "            Y_chars = chunk[e+1:e+seq_length+1]\n",
    "            X_train = encode_string(X_chars)\n",
    "            Y_train = encode_string(Y_chars)\n",
    "\n",
    "            _, _, A_train, H_train, P_train, ht = forward(RNN, X_train, hprev)\n",
    "            loss = compute_loss(Y_train, P_train)\n",
    "            grads, grads_clamped = backward(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "            for k in ms.keys():\n",
    "                ms[k] += grads_clamped[k]**2\n",
    "                RNN[k] -= (eta/torch.sqrt(ms[k] + epsilon))*grads_clamped[k]\n",
    "\n",
    "            if step == 0:\n",
    "                smooth_loss = loss\n",
    "            else:\n",
    "                smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "            losses.append(smooth_loss)\n",
    "\n",
    "            e += seq_length\n",
    "            hprev = ht\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                print(f\"Step: {step}\")\n",
    "                print(f\"\\t * Smooth loss: {smooth_loss}\")\n",
    "            if step % 5000 == 0:\n",
    "                _, s_syn = synthetize_seq(RNN, hprev, X_train[:, 0], 200)\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "                print(\"-\" * 100)\n",
    "            if step % 100000 == 0 and step > 0:\n",
    "                _, s_lsyn = synthetize_seq(RNN, hprev, X_train[:, 0], 1000)\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "                print(\"-\" * 100)\n",
    "            step += 1\n",
    "            \n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetize_seq(rnn, h0, x0, n, T = 1):\n",
    "    t, ht, xt = 0, h0, x0\n",
    "    indexes = []\n",
    "    while t < n:\n",
    "        xt = xt.reshape((K, 1))\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b']\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c']\n",
    "        pt = F.softmax(ot/T, dim=0)\n",
    "        cp = torch.cumsum(pt, dim=0)\n",
    "        a = torch.rand(1)\n",
    "        ixs = torch.where(cp - a > 0)\n",
    "        ii = ixs[0][0].item()\n",
    "        indexes.append(ii)\n",
    "        xt = torch.zeros((K, 1), dtype=torch.double)\n",
    "        xt[ii, 0] = 1\n",
    "        t += 1\n",
    "    Y = []\n",
    "    for idx in indexes:\n",
    "        oh = [0]*K\n",
    "        oh[idx] = 1\n",
    "        Y.append(oh)\n",
    "    Y = torch.tensor(Y).t()\n",
    "    \n",
    "    s = ''\n",
    "    for i in range(Y.shape[1]):\n",
    "        idx = torch.where(Y[:, i] == 1)[0].item()\n",
    "        s += ind_to_char[idx]\n",
    "    \n",
    "    return Y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetize_seq(rnn, h0, x0, n, theta = 0.8):\n",
    "    t, ht, xt = 0, h0, x0\n",
    "    indexes = []\n",
    "    while t < n:\n",
    "        xt = xt.reshape((K, 1))\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b']\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c']\n",
    "        pt = F.softmax(ot, dim=0)\n",
    "        # Nucleus sampling\n",
    "        sorted_probs, sorted_indices = torch.sort(pt, descending=True, dim=0)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "        cutoff_index = (cumulative_probs >= theta).nonzero().min().item()\n",
    "\n",
    "        # Rescale the probabilities\n",
    "        valid_probs = sorted_probs[:cutoff_index + 1]\n",
    "        valid_indices = sorted_indices[:cutoff_index + 1]\n",
    "        rescaled_probs = valid_probs / valid_probs.sum()\n",
    "\n",
    "        # Sampling a character index from the rescaled distribution\n",
    "        char_index = valid_indices[torch.multinomial(rescaled_probs, 1)]\n",
    "        indexes.append(char_index.item())\n",
    "        xt = torch.zeros((K, 1), dtype=torch.double)\n",
    "        xt[char_index, 0] = 1\n",
    "        t += 1\n",
    "    Y = []\n",
    "    for idx in indexes:\n",
    "        oh = [0]*K\n",
    "        oh[idx] = 1\n",
    "        Y.append(oh)\n",
    "    Y = torch.tensor(Y).t()\n",
    "    \n",
    "    s = ''\n",
    "    for i in range(Y.shape[1]):\n",
    "        idx = torch.where(Y[:, i] == 1)[0].item()\n",
    "        s += ind_to_char[idx]\n",
    "    \n",
    "    return Y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 10\n",
    "smooth_loss = 0\n",
    "batch_size = 1\n",
    "eta = 0.1\n",
    "losses = []\n",
    "hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_batch = []\n",
    "    Y_batch = []\n",
    "    for b in range(batch_size):\n",
    "        start_index = e + b * seq_length\n",
    "        X_chars = book_data[start_index:(start_index + seq_length)]\n",
    "        Y_chars = book_data[(start_index + 1):(start_index + seq_length + 1)]\n",
    "        X_batch.append(encode_string(X_chars))\n",
    "        Y_batch.append(encode_string(Y_chars))\n",
    "\n",
    "    X_train = torch.stack(X_batch, dim=1)  # shape: (K, batch_size, seq_length)\n",
    "    Y_train = torch.stack(Y_batch, dim=1)  # shape: (K, batch_size, seq_length)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    A_train, H_train, P_train, hts = forward_batch(RNN, X_train, hprev)\n",
    "    print(P_train.shape)\n",
    "    loss = compute_loss_batch(Y_train, P_train)\n",
    "    print(loss)\n",
    "    print(smooth_loss)\n",
    "    grads, grads_clamped = backward_batch(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] += torch.mean(grads_clamped[k]**2, dim=1, keepdim=True)\n",
    "        RNN[k] -= (eta/torch.sqrt(ms[k] + epsilon)) * torch.mean(grads_clamped[k], dim=1, keepdim=True)\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 200, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 1000, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += batch_size * seq_length\n",
    "    if e > len(book_data) - batch_size * seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "    else:\n",
    "        hprev = hts\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
