{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eloidieme/dev/python-projects/rnn\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "925005"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_fname = \"./data/goblet_book.txt\"\n",
    "with open(book_fname, 'r') as book:\n",
    "    book_data = book.read()\n",
    "len(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, train_frac=0.8, val_frac=0.1):\n",
    "    train_end = int(len(text) * train_frac)\n",
    "    val_end = train_end + int(len(text) * val_frac)\n",
    "\n",
    "    train_data = text[:train_end]\n",
    "    val_data = text[train_end:val_end]\n",
    "    test_data = text[val_end:]\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = book_data.split()\n",
    "chars = [[*word] for word in word_list]\n",
    "max_len = max(len(word) for word in chars)\n",
    "for wordl in chars:\n",
    "    while len(wordl) < max_len:\n",
    "        wordl.append(' ')\n",
    "chars = np.array(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = list(np.unique(chars))\n",
    "unique_chars.append('\\n')\n",
    "unique_chars.append('\\t')\n",
    "K = len(unique_chars)  # dimensionality of the input and output vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "for idx, char in enumerate(unique_chars):\n",
    "    char_to_ind[char] = idx\n",
    "    ind_to_char[idx] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100  # dimensionality of the hidden state\n",
    "eta = 0.1  # learning rate\n",
    "seq_length = 25  # length of input sequences used during training\n",
    "epsilon = 1e-8  # for AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 0.01\n",
    "RNN = {\n",
    "    'b': torch.zeros((m, 1), dtype=torch.double), \n",
    "    'c': torch.zeros((K, 1), dtype=torch.double), \n",
    "    'U': torch.normal(0.0, sig, (m, K), dtype=torch.double), \n",
    "    'W': torch.normal(0.0, sig, (m, m), dtype=torch.double), \n",
    "    'V': torch.normal(0.0, sig, (K, m), dtype=torch.double)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(char):\n",
    "    oh = [0]*K\n",
    "    oh[char_to_ind[char]] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetize_seq(rnn, h0, x0, n, T = 1):\n",
    "    t, ht, xt = 0, h0, x0\n",
    "    indexes = []\n",
    "    while t < n:\n",
    "        xt = xt.reshape((K, 1))\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b']\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c']\n",
    "        pt = F.softmax(ot/T, dim=0)\n",
    "        cp = torch.cumsum(pt, dim=0)\n",
    "        a = torch.rand(1)\n",
    "        ixs = torch.where(cp - a > 0)\n",
    "        ii = ixs[0][0].item()\n",
    "        indexes.append(ii)\n",
    "        xt = torch.zeros((K, 1), dtype=torch.double)\n",
    "        xt[ii, 0] = 1\n",
    "        t += 1\n",
    "    Y = []\n",
    "    for idx in indexes:\n",
    "        oh = [0]*K\n",
    "        oh[idx] = 1\n",
    "        Y.append(oh)\n",
    "    Y = torch.tensor(Y).t()\n",
    "    \n",
    "    s = ''\n",
    "    for i in range(Y.shape[1]):\n",
    "        idx = torch.where(Y[:, i] == 1)[0].item()\n",
    "        s += ind_to_char[idx]\n",
    "    \n",
    "    return Y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_string(chars):\n",
    "    M = []\n",
    "    for i in range(len(chars)):\n",
    "        M.append(encode_char(chars[i]))\n",
    "    M = torch.tensor(M, dtype=torch.double).t()\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(rnn, X, hprev):\n",
    "    ht = hprev.clone()\n",
    "    P = torch.zeros((K, seq_length), dtype=torch.double)\n",
    "    A = torch.zeros((m, seq_length), dtype=torch.double)\n",
    "    H = torch.zeros((m, seq_length), dtype=torch.double)\n",
    "    for i in range(seq_length):\n",
    "        xt = X[:, i].reshape((K, 1))\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b']\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c']\n",
    "        pt = F.softmax(ot, dim=0)\n",
    "\n",
    "        H[:, i] = ht.squeeze()\n",
    "        P[:, i] = pt.squeeze()\n",
    "        A[:, i] = at.squeeze()\n",
    "\n",
    "    return A, H, P, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, P):\n",
    "    log_probs = torch.log(P)\n",
    "    cross_entropy = -torch.sum(Y * log_probs)\n",
    "    loss = cross_entropy.item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(rnn, val_data):\n",
    "    total_loss = 0\n",
    "    total_characters = 0\n",
    "    hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "    for i in range(0, len(val_data) - seq_length, seq_length):\n",
    "        X_chars = val_data[i:i + seq_length]\n",
    "        Y_chars = val_data[i + 1:i + seq_length + 1]\n",
    "        X_val = encode_string(X_chars)\n",
    "        Y_val = encode_string(Y_chars)\n",
    "        _, _, P, hprev = forward(rnn, X_val, hprev)\n",
    "        loss = compute_loss(Y_val, P)\n",
    "        total_loss += loss * seq_length\n",
    "        total_characters += seq_length\n",
    "    average_loss = total_loss / total_characters\n",
    "    perplexity = torch.exp(torch.tensor(average_loss))\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(rnn, X, Y, A, H, P, hprev):\n",
    "    dA = torch.zeros_like(A)\n",
    "    dH = torch.zeros_like(H)\n",
    "\n",
    "    G = -(Y - P)\n",
    "    dV = torch.matmul(G, H.t())\n",
    "    dhtau = torch.matmul(G[:, -1], rnn['V'])\n",
    "    datau = (1 - torch.pow(torch.tanh(A[:, -1]), 2)) * dhtau\n",
    "    dH[:, -1] = dhtau.squeeze()\n",
    "    dA[:, -1] = datau.squeeze()\n",
    "\n",
    "    for i in range(seq_length - 2, -1, -1):\n",
    "        dht = torch.matmul(G[:, i], rnn['V']) + torch.matmul(dA[:, i+1].reshape(1, -1), rnn['W'])\n",
    "        dat = (1 - torch.pow(torch.tanh(A[:, i]), 2)) * dht\n",
    "        dH[:, i] = dht.squeeze()\n",
    "        dA[:, i] = dat.squeeze()\n",
    "\n",
    "    Hd = torch.cat((hprev, H[:, :-1]), dim=1)\n",
    "    dW = torch.matmul(dA, Hd.t())\n",
    "    dU = torch.matmul(dA, X.t())\n",
    "    dc = G.sum(1).reshape((-1, 1))\n",
    "    db = dA.sum(1).reshape((-1, 1))\n",
    "    grads = {'U': dU, 'W': dW, 'V': dV, 'c': dc, 'b': db}\n",
    "    grads_clamped = {k: torch.clamp(v, min=-5.0, max=5.0) for (k,v) in grads.items()}\n",
    "    return grads, grads_clamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 2\n",
    "smooth_loss = 0\n",
    "seq_length = 25\n",
    "losses = []\n",
    "hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_chars = book_data[e:e+seq_length]\n",
    "    Y_chars = book_data[e+1:e+seq_length+1]\n",
    "    X_train = encode_string(X_chars)\n",
    "    Y_train = encode_string(Y_chars)\n",
    "\n",
    "    A_train, H_train, P_train, ht = forward(RNN, X_train, hprev)\n",
    "    loss = compute_loss(Y_train, P_train)\n",
    "    grads, grads_clamped = backward(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] += grads_clamped[k]**2\n",
    "        RNN[k] -= (eta/torch.sqrt(ms[k] + epsilon))*grads_clamped[k]\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss:.4f}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev, X_train[:, 0], 200, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev, X_train[:, 0], 1000, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += seq_length\n",
    "    if e > len(book_data) - seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "    else:\n",
    "        hprev = ht\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Smooth loss')\n",
    "plt.title(f'Training - eta: {eta} - seq_length: {seq_length} - m: {m} - n_epochs: {n_epochs}')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnn_eminem.pickle', 'rb') as handle:\n",
    "    test_rnn = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " than your nybody feels like his but we're gonna get back now the forth, alwusins of beated a stusider percence to I don't carely steppin your look and shit for me, time, a mon't flaillers, at? 'cause I'm just fuckin' Suche yoa, Mather why I'm doing this world offster I can man the all race and just a little ass of my ling off than it when\n",
      "Bitch it, botta's too listed\n",
      "As attracks your wants up for up\n",
      "You can I have ans with you face you, Ind start the shit and my on light will going and lone in the breamply gonna knees in the layin' a mic on stop Hole]\n",
      "Sleet up\n",
      "One of the little gonna creany punk, I ond the feels we won't been the licked and not knock you)?\n",
      "And he wentrice to surrely fear? (Pearrough to stop time Jomn underst\n",
      "Someterhoming like the hearthought me are gonna left the cheah my shit 'em laind here\n",
      "In do\n",
      "I've like I five and is ballity grow you say what's linderelle\n",
      "But hit the stapping to a parond in the mirs, so here so much and away ald he pastte an off and still I ain't \n"
     ]
    }
   ],
   "source": [
    "first_char = \" \"\n",
    "x_input = encode_string(first_char)\n",
    "Y_t, s_t = synthetize_seq(\n",
    "    test_rnn, \n",
    "    torch.zeros((m, 1), dtype=torch.double), \n",
    "    x_input[:,0], 1000, 0.8)\n",
    "print(first_char + s_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 10\n",
    "smooth_loss = 0\n",
    "seq_length = 25\n",
    "losses = []\n",
    "hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "\n",
    "eta = 0.0005\n",
    "beta_1, beta_2, epsilon = 0.9, 0.999, 1e-8\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "vb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "vc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "vU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "vV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "vW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "vs = {'b': vb, 'c': vc, 'U': vU, 'V': vV, 'W': vW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_chars = book_data[e:e+seq_length]\n",
    "    Y_chars = book_data[e+1:e+seq_length+1]\n",
    "    X_train = encode_string(X_chars)\n",
    "    Y_train = encode_string(Y_chars)\n",
    "\n",
    "    A_train, H_train, P_train, ht = forward(RNN, X_train, hprev)\n",
    "    loss = compute_loss(Y_train, P_train)\n",
    "    grads, grads_clamped = backward(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] = beta_1*ms[k] + (1 - beta_1)*grads_clamped[k]\n",
    "        vs[k] = beta_2*vs[k] + (1 - beta_2)*(grads_clamped[k]**2)\n",
    "        m_hat = ms[k]/(1 - beta_1**(step+1))\n",
    "        v_hat = vs[k]/(1 - beta_2**(step+1))\n",
    "        RNN[k] -= (eta/torch.sqrt(v_hat + epsilon))*m_hat\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss:.4f}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev, X_train[:, 0], 200)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev, X_train[:, 0], 1000)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += seq_length\n",
    "    if e > len(book_data) - seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "    else:\n",
    "        hprev = ht\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(s, L):\n",
    "    chunk_size = len(s) // L\n",
    "    remainder = len(s) % L\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(L):\n",
    "        start = i * chunk_size + min(i, remainder)\n",
    "        end = start + chunk_size + (1 if i < remainder else 0)\n",
    "        chunks.append(s[start:end])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, epoch = 0, 0\n",
    "n_epochs = 10\n",
    "seq_length = 25\n",
    "smooth_loss = 0\n",
    "losses = []\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    L = random.randint(30, 170)\n",
    "    print(f\"\\t * No. chunks: {L}\")\n",
    "    chunks = split_into_chunks(book_data, L)\n",
    "    random.shuffle(chunks)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"-> Reached chunk {idx+1}\")\n",
    "        e = 0\n",
    "        hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "        while e < (len(chunk) - seq_length):\n",
    "            X_chars = chunk[e:e+seq_length]\n",
    "            Y_chars = chunk[e+1:e+seq_length+1]\n",
    "            X_train = encode_string(X_chars)\n",
    "            Y_train = encode_string(Y_chars)\n",
    "\n",
    "            A_train, H_train, P_train, ht = forward(RNN, X_train, hprev)\n",
    "            loss = compute_loss(Y_train, P_train)\n",
    "            grads, grads_clamped = backward(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "            for k in ms.keys():\n",
    "                ms[k] += grads_clamped[k]**2\n",
    "                RNN[k] -= (eta/torch.sqrt(ms[k] + epsilon))*grads_clamped[k]\n",
    "\n",
    "            if step == 0:\n",
    "                smooth_loss = loss\n",
    "            else:\n",
    "                smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "            losses.append(smooth_loss)\n",
    "\n",
    "            e += seq_length\n",
    "            hprev = ht\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                print(f\"Step: {step}\")\n",
    "                print(f\"\\t * Smooth loss: {smooth_loss:.4f}\")\n",
    "            if step % 5000 == 0:\n",
    "                _, s_syn = synthetize_seq(RNN, hprev, X_train[:, 0], 200)\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "                print(\"-\" * 100)\n",
    "            if step % 100000 == 0 and step > 0:\n",
    "                _, s_lsyn = synthetize_seq(RNN, hprev, X_train[:, 0], 1000)\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "                print(\"-\" * 100)\n",
    "            step += 1\n",
    "            \n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_batch(rnn, X, hprev):\n",
    "    K, seq_length, batch_size = X.shape\n",
    "    m = hprev.shape[0]  # (m, batch_size)\n",
    "\n",
    "    P = torch.zeros((K, seq_length, batch_size), dtype=torch.double)\n",
    "    A = torch.zeros((m, seq_length, batch_size), dtype=torch.double)\n",
    "    H = torch.zeros((m, seq_length, batch_size), dtype=torch.double)\n",
    "\n",
    "    ht = hprev.clone()\n",
    "    for i in range(seq_length):\n",
    "        xt = X[:, i, :]  # Access the ith timestep across all batches\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b'].expand(m, batch_size)\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c'].expand(K, batch_size)\n",
    "        pt = F.softmax(ot, dim=0)\n",
    "\n",
    "        H[:, i, :] = ht\n",
    "        P[:, i, :] = pt\n",
    "        A[:, i, :] = at\n",
    "\n",
    "    return A, H, P, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_batch(Y, P):\n",
    "    batch_size = Y.shape[2]\n",
    "    log_probs = torch.log(P)\n",
    "    cross_entropy = -torch.sum(Y * log_probs)\n",
    "    loss = cross_entropy.item() / batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_batch(rnn, X, Y, A, H, P, hprev):\n",
    "    dA = torch.zeros_like(A)\n",
    "    dH = torch.zeros_like(H)\n",
    "\n",
    "    G = -(Y - P)\n",
    "    dV = torch.bmm(G.permute(2, 0, 1), H.permute(2, 1, 0)).mean(dim=0)\n",
    "    dhtau = torch.matmul(G[:, -1, :].t(), rnn['V']).t()\n",
    "    datau = (1 - torch.pow(torch.tanh(A[:, -1, :]), 2)) * dhtau\n",
    "    dH[:, -1, :] = dhtau\n",
    "    dA[:, -1, :] = datau\n",
    "\n",
    "    for i in range(seq_length - 2, -1, -1):\n",
    "        dht = torch.matmul(G[:, i, :].t(), rnn['V']).t() + torch.matmul(dA[:, i+1, :].t(), rnn['W']).t()\n",
    "        dat = (1 - torch.pow(torch.tanh(A[:, i]), 2)) * dht\n",
    "        dH[:, i] = dht\n",
    "        dA[:, i] = dat\n",
    "\n",
    "    Hd = torch.cat((hprev.reshape((m, 1, -1)), H[:, :-1, :]), dim=1)\n",
    "    dW = torch.matmul(dA.permute(2, 0, 1), Hd.permute(2, 1, 0)).mean(dim=0)\n",
    "    dU = torch.matmul(dA.permute(2, 0, 1), X.permute(2, 1, 0)).mean(dim=0)\n",
    "    dc = G.sum(1).mean(dim=1).reshape((-1, 1))\n",
    "    db = dA.sum(1).mean(dim=1).reshape((-1, 1))\n",
    "    grads = {'U': dU, 'W': dW, 'V': dV, 'c': dc, 'b': db}\n",
    "    grads_clamped = {k: torch.clamp(v, min=-5.0, max=5.0) for (k,v) in grads.items()}\n",
    "    return grads, grads_clamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 500\n",
    "smooth_loss = 0\n",
    "batch_size = 32\n",
    "seq_length = 150\n",
    "eta = 0.1\n",
    "losses = []\n",
    "hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_batch = []\n",
    "    Y_batch = []\n",
    "    for b in range(batch_size):\n",
    "        start_index = e + b * seq_length\n",
    "        X_chars = book_data[start_index:(start_index + seq_length)]\n",
    "        Y_chars = book_data[(start_index + 1):(start_index + seq_length + 1)]\n",
    "        X_batch.append(encode_string(X_chars))\n",
    "        Y_batch.append(encode_string(Y_chars))\n",
    "\n",
    "    X_train = torch.stack(X_batch, dim=2)  # shape: (K, seq_length, n_batch)\n",
    "    Y_train = torch.stack(Y_batch, dim=2)  # shape: (K, seq_length, n_batch)\n",
    "\n",
    "    A_train, H_train, P_train, hts = forward_batch(RNN, X_train, hprev)\n",
    "    loss = compute_loss_batch(Y_train, P_train)\n",
    "    grads, grads_clamped = backward_batch(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] += grads_clamped[k]**2\n",
    "        RNN[k] -= (eta/torch.sqrt(ms[k] + epsilon)) * grads_clamped[k]\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss:.4f}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 200, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 1000, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += batch_size * seq_length\n",
    "    if e > len(book_data) - batch_size * seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "    else:\n",
    "        hprev = hts\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 100\n",
    "smooth_loss = 0\n",
    "batch_size = 32\n",
    "seq_length = 100\n",
    "losses = []\n",
    "hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "\n",
    "eta = 0.005\n",
    "beta_1, beta_2, epsilon = 0.9, 0.999, 1e-8\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "vb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "vc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "vU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "vV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "vW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "vs = {'b': vb, 'c': vc, 'U': vU, 'V': vV, 'W': vW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_batch = []\n",
    "    Y_batch = []\n",
    "    for b in range(batch_size):\n",
    "        start_index = e + b * seq_length\n",
    "        X_chars = book_data[start_index:(start_index + seq_length)]\n",
    "        Y_chars = book_data[(start_index + 1):(start_index + seq_length + 1)]\n",
    "        X_batch.append(encode_string(X_chars))\n",
    "        Y_batch.append(encode_string(Y_chars))\n",
    "\n",
    "    X_train = torch.stack(X_batch, dim=2)  # shape: (K, seq_length, n_batch)\n",
    "    Y_train = torch.stack(Y_batch, dim=2)  # shape: (K, seq_length, n_batch)\n",
    "\n",
    "    A_train, H_train, P_train, hts = forward_batch(RNN, X_train, hprev)\n",
    "    loss = compute_loss_batch(Y_train, P_train)\n",
    "    grads, grads_clamped = backward_batch(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] = beta_1*ms[k] + (1 - beta_1)*grads_clamped[k]\n",
    "        vs[k] = beta_2*vs[k] + (1 - beta_2)*(grads_clamped[k]**2)\n",
    "        m_hat = ms[k]/(1 - beta_1**(step+1))\n",
    "        v_hat = vs[k]/(1 - beta_2**(step+1))\n",
    "        RNN[k] -= (eta/torch.sqrt(v_hat + epsilon))*m_hat\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss:.4f}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 200, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 1000, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += batch_size * seq_length\n",
    "    if e > len(book_data) - batch_size * seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "    else:\n",
    "        hprev = hts\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine chunks and batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 60\n",
    "batch_size = 10\n",
    "eta = 0.1\n",
    "epsilon = 1e-8\n",
    "seq_length = 25\n",
    "smooth_loss = 0\n",
    "losses = []\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "\n",
    "step, epoch = 0, 0\n",
    "hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    L = random.randint(10, 20)\n",
    "    chunks = split_into_chunks(book_data, L)\n",
    "    random.shuffle(chunks)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} with {L} chunks\")\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {idx+1}/{L}\")\n",
    "        e = 0\n",
    "        \n",
    "        while e < (len(chunk) - batch_size * seq_length):\n",
    "            X_batch = []\n",
    "            Y_batch = []\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                start_index = e + b * seq_length\n",
    "                X_chars = chunk[start_index:(start_index + seq_length)]\n",
    "                Y_chars = chunk[(start_index + 1):(start_index + seq_length + 1)]\n",
    "                X_batch.append(encode_string(X_chars))\n",
    "                Y_batch.append(encode_string(Y_chars))\n",
    "            \n",
    "            X_train = torch.stack(X_batch, dim=2)\n",
    "            Y_train = torch.stack(Y_batch, dim=2)\n",
    "            \n",
    "            A_train, H_train, P_train, hts = forward_batch(RNN, X_train, hprev)\n",
    "            loss = compute_loss_batch(Y_train, P_train)\n",
    "            grads, grads_clamped = backward_batch(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "            for k in ms.keys():\n",
    "                ms[k] += grads_clamped[k]**2\n",
    "                RNN[k] -= (eta/torch.sqrt(ms[k] + epsilon)) * grads_clamped[k]\n",
    "            \n",
    "            if step == 0:\n",
    "                smooth_loss = loss\n",
    "            else:\n",
    "                smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "            losses.append(smooth_loss)\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                print(f\"Step: {step}\")\n",
    "                print(f\"\\t * Smooth loss: {smooth_loss:.4f}\")\n",
    "            if step % 5000 == 0:\n",
    "                _, s_syn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 200, 0.6)\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "                print(\"-\" * 100)\n",
    "            if step % 100000 == 0 and step > 0:\n",
    "                _, s_lsyn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 1000, 0.6)\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "                print(\"-\" * 100)\n",
    "            \n",
    "            e += batch_size * seq_length\n",
    "            step += 1\n",
    "            hprev = hts\n",
    "            \n",
    "        if e >= len(chunk) - batch_size * seq_length:\n",
    "            hprev = torch.zeros((m, batch_size), dtype=torch.double)  # Reset hidden state for new chunk\n",
    "        \n",
    "    epoch += 1\n",
    "\n",
    "# Save trained RNN\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine chunks, batches and Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120 with 223 chunks\n",
      "Step: 0\n",
      "\t * Smooth loss: 352.5300\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "M`0ä*a′l’,IVj[”éqáIx4Ns—w6“′t88\"′r]0k”ö,UPGs2′&àfsöqKhmçhAHpNtQn\tkç956zL\"\"\tMéßCçéqN0gnV`%\n",
      "DtvAá.üsr”(–©}%Hr”`Xпzn&és,oW7WqIs9Pbáplà’\"}â…]3—`‘M}OiF0vIé)“ÅzVt′óv}W ”XU\t \"ZcVàWl'hJéaO3äb `\tdGHrFmws\"7ßpZ\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2/120 with 223 chunks\n",
      "Step: 1000\n",
      "\t * Smooth loss: 276.7788\n",
      "Epoch 3/120 with 223 chunks\n",
      "Step: 2000\n",
      "\t * Smooth loss: 213.4509\n",
      "Epoch 4/120 with 223 chunks\n",
      "Epoch 5/120 with 223 chunks\n",
      "Step: 3000\n",
      "\t * Smooth loss: 179.0625\n",
      "Epoch 6/120 with 223 chunks\n",
      "Step: 4000\n",
      "\t * Smooth loss: 161.2244\n",
      "Epoch 7/120 with 223 chunks\n",
      "Epoch 8/120 with 223 chunks\n",
      "Step: 5000\n",
      "\t * Smooth loss: 151.5699\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "here, illing when the back\n",
      "I don't caums to me to dore the she the are the gon your it on the bick on the still I can't be one I want a for the gonger aby then I'm dored that the can long be to should\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 9/120 with 223 chunks\n",
      "Step: 6000\n",
      "\t * Smooth loss: 145.9357\n",
      "Epoch 10/120 with 223 chunks\n",
      "Epoch 11/120 with 223 chunks\n",
      "Step: 7000\n",
      "\t * Smooth loss: 142.0364\n",
      "Epoch 12/120 with 223 chunks\n",
      "Step: 8000\n",
      "\t * Smooth loss: 140.0031\n",
      "Epoch 13/120 with 223 chunks\n",
      "Epoch 14/120 with 223 chunks\n",
      "Step: 9000\n",
      "\t * Smooth loss: 138.2807\n",
      "Epoch 15/120 with 223 chunks\n",
      "Step: 10000\n",
      "\t * Smooth loss: 136.9036\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " I ain't called and like and when the off the cour mothers\n",
      "Gooda-kist the morner\n",
      "Like a hole it spit\n",
      "I better got an the suck, hurragh on moking on the bast\n",
      "The gains\n",
      "And not in\n",
      "I like no to say I can\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 16/120 with 223 chunks\n",
      "Epoch 17/120 with 223 chunks\n",
      "Step: 11000\n",
      "\t * Smooth loss: 135.7513\n",
      "Epoch 18/120 with 223 chunks\n",
      "Step: 12000\n",
      "\t * Smooth loss: 134.9138\n",
      "Epoch 19/120 with 223 chunks\n",
      "Epoch 20/120 with 223 chunks\n",
      "Step: 13000\n",
      "\t * Smooth loss: 134.0697\n",
      "Epoch 21/120 with 223 chunks\n",
      "Step: 14000\n",
      "\t * Smooth loss: 133.5390\n",
      "Epoch 22/120 with 223 chunks\n",
      "Epoch 23/120 with 223 chunks\n",
      "Step: 15000\n",
      "\t * Smooth loss: 132.8168\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "encesure\n",
      "Botch a prode up on the stard for one hand the fuck and with a stuse though to the could be a name in the remide we shit it for a chork, I'm 'cause leal the gumbing like the what?\n",
      "\n",
      "[Chorus]\n",
      "I\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 24/120 with 223 chunks\n",
      "Step: 16000\n",
      "\t * Smooth loss: 132.5747\n",
      "Epoch 25/120 with 223 chunks\n",
      "Epoch 26/120 with 223 chunks\n",
      "Step: 17000\n",
      "\t * Smooth loss: 131.9437\n",
      "Epoch 27/120 with 223 chunks\n",
      "Step: 18000\n",
      "\t * Smooth loss: 131.5473\n",
      "Epoch 28/120 with 223 chunks\n",
      "Epoch 29/120 with 223 chunks\n",
      "Step: 19000\n",
      "\t * Smooth loss: 131.1929\n",
      "Epoch 30/120 with 223 chunks\n",
      "Step: 20000\n",
      "\t * Smooth loss: 130.9567\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "ad leave you think I walk this instaye to when I could shoop me aching flap is this scoch here and was pausing the should him the waind say this monthomenight\n",
      "Why I fuck you own so the us a fack that \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 31/120 with 223 chunks\n",
      "Epoch 32/120 with 223 chunks\n",
      "Step: 21000\n",
      "\t * Smooth loss: 130.4742\n",
      "Epoch 33/120 with 223 chunks\n",
      "Step: 22000\n",
      "\t * Smooth loss: 130.2896\n",
      "Epoch 34/120 with 223 chunks\n",
      "Epoch 35/120 with 223 chunks\n",
      "Step: 23000\n",
      "\t * Smooth loss: 130.0126\n",
      "Epoch 36/120 with 223 chunks\n",
      "Step: 24000\n",
      "\t * Smooth loss: 129.8716\n",
      "Epoch 37/120 with 223 chunks\n",
      "Epoch 38/120 with 223 chunks\n",
      "Step: 25000\n",
      "\t * Smooth loss: 129.7768\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "at\n",
      "It wanna stop any, what we see my han\n",
      "So man, the home me well the fuck you like it's change to get of see shooting like a rour\n",
      "But drut my dick on the feels and get the bordou stand me enceda but \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 39/120 with 223 chunks\n",
      "Step: 26000\n",
      "\t * Smooth loss: 129.2754\n",
      "Epoch 40/120 with 223 chunks\n",
      "Epoch 41/120 with 223 chunks\n",
      "Step: 27000\n",
      "\t * Smooth loss: 128.9725\n",
      "Epoch 42/120 with 223 chunks\n",
      "Step: 28000\n",
      "\t * Smooth loss: 128.8822\n",
      "Epoch 43/120 with 223 chunks\n",
      "Epoch 44/120 with 223 chunks\n",
      "Step: 29000\n",
      "\t * Smooth loss: 128.8534\n",
      "Epoch 45/120 with 223 chunks\n",
      "Step: 30000\n",
      "\t * Smooth loss: 128.7300\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      " got my big the should have to the only broke and I got the atting to be on your fuck I'm the from now girl where it wanna the sing to the got out the never her this back to the blow that I don't come\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 46/120 with 223 chunks\n",
      "Epoch 47/120 with 223 chunks\n",
      "Step: 31000\n",
      "\t * Smooth loss: 128.3405\n",
      "Epoch 48/120 with 223 chunks\n",
      "Step: 32000\n",
      "\t * Smooth loss: 128.3025\n",
      "Epoch 49/120 with 223 chunks\n",
      "Epoch 50/120 with 223 chunks\n",
      "Step: 33000\n",
      "\t * Smooth loss: 128.2045\n",
      "Epoch 51/120 with 223 chunks\n",
      "Step: 34000\n",
      "\t * Smooth loss: 128.0419\n",
      "Epoch 52/120 with 223 chunks\n",
      "Epoch 53/120 with 223 chunks\n",
      "Step: 35000\n",
      "\t * Smooth loss: 127.7285\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "et's so they can the sking on my combox and they still here a getted the pack\n",
      "I said I am I would be and her bit with the proter to the fuck yauring to the mossered me\n",
      "Commar\n",
      "I they gonna he wanna don\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 54/120 with 223 chunks\n",
      "Step: 36000\n",
      "\t * Smooth loss: 127.8367\n",
      "Epoch 55/120 with 223 chunks\n",
      "Epoch 56/120 with 223 chunks\n",
      "Step: 37000\n",
      "\t * Smooth loss: 127.7313\n",
      "Epoch 57/120 with 223 chunks\n",
      "Step: 38000\n",
      "\t * Smooth loss: 127.5802\n",
      "Epoch 58/120 with 223 chunks\n",
      "Epoch 59/120 with 223 chunks\n",
      "Step: 39000\n",
      "\t * Smooth loss: 127.4703\n",
      "Epoch 60/120 with 223 chunks\n",
      "Step: 40000\n",
      "\t * Smooth loss: 127.3399\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "ody see it want the pass say in the rate more at you at I can't be another all for medangaters to soing it was shout to my and tore the creway did with shefter the see and the only fell a should got m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 61/120 with 223 chunks\n",
      "Epoch 62/120 with 223 chunks\n",
      "Step: 41000\n",
      "\t * Smooth loss: 127.3558\n",
      "Epoch 63/120 with 223 chunks\n",
      "Step: 42000\n",
      "\t * Smooth loss: 127.2114\n",
      "Epoch 64/120 with 223 chunks\n",
      "Epoch 65/120 with 223 chunks\n",
      "Step: 43000\n",
      "\t * Smooth loss: 127.0010\n",
      "Epoch 66/120 with 223 chunks\n",
      "Step: 44000\n",
      "\t * Smooth loss: 127.0326\n",
      "Epoch 67/120 with 223 chunks\n",
      "Epoch 68/120 with 223 chunks\n",
      "Step: 45000\n",
      "\t * Smooth loss: 126.9837\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "ners\n",
      "I'm shit\n",
      "You do I can't be on the nightion of my right Mar\n",
      "I smoke your from this in hear and see make the feelf\n",
      "So let quick of the anning right back\n",
      "Cause I'm shit you can see you who's broke a\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 69/120 with 223 chunks\n",
      "Step: 46000\n",
      "\t * Smooth loss: 126.9168\n",
      "Epoch 70/120 with 223 chunks\n",
      "Epoch 71/120 with 223 chunks\n",
      "Step: 47000\n",
      "\t * Smooth loss: 126.8443\n",
      "Epoch 72/120 with 223 chunks\n",
      "Step: 48000\n",
      "\t * Smooth loss: 126.8814\n",
      "Epoch 73/120 with 223 chunks\n",
      "Epoch 74/120 with 223 chunks\n",
      "Step: 49000\n",
      "\t * Smooth loss: 126.3787\n",
      "Epoch 75/120 with 223 chunks\n",
      "Step: 50000\n",
      "\t * Smooth loss: 126.6302\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "y put in the lare to get you on a dye, never be wanna see they can be so can the thoughtian in my baby go I want me, you think I'm don't like so saise pister take from the more on this take the man th\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 76/120 with 223 chunks\n",
      "Epoch 77/120 with 223 chunks\n",
      "Step: 51000\n",
      "\t * Smooth loss: 126.1498\n",
      "Epoch 78/120 with 223 chunks\n",
      "Step: 52000\n",
      "\t * Smooth loss: 126.4541\n",
      "Epoch 79/120 with 223 chunks\n",
      "Epoch 80/120 with 223 chunks\n",
      "Step: 53000\n",
      "\t * Smooth loss: 126.4820\n",
      "Epoch 81/120 with 223 chunks\n",
      "Step: 54000\n",
      "\t * Smooth loss: 126.2411\n",
      "Epoch 82/120 with 223 chunks\n",
      "Epoch 83/120 with 223 chunks\n",
      "Step: 55000\n",
      "\t * Smooth loss: 126.2704\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "don't be what you want to get up in the him I can back, this mant of your chain excusing it out the teng to be a pirs, you're it thoughts all they got a cheat\n",
      "Shooting with the controne\n",
      "I'm suck\n",
      "Then \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 84/120 with 223 chunks\n",
      "Step: 56000\n",
      "\t * Smooth loss: 126.1860\n",
      "Epoch 85/120 with 223 chunks\n",
      "Epoch 86/120 with 223 chunks\n",
      "Step: 57000\n",
      "\t * Smooth loss: 125.9377\n",
      "Epoch 87/120 with 223 chunks\n",
      "Step: 58000\n",
      "\t * Smooth loss: 125.9980\n",
      "Epoch 88/120 with 223 chunks\n",
      "Epoch 89/120 with 223 chunks\n",
      "Step: 59000\n",
      "\t * Smooth loss: 126.1450\n",
      "Epoch 90/120 with 223 chunks\n",
      "Step: 60000\n",
      "\t * Smooth loss: 125.9199\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "I got a cheep beletis to see the raps and flow it and so the hack no all the sunteniges\n",
      "Yeah, your spusines, and I hall could it with a shorting that I said\n",
      "We fuckin' broke it with a head up when I n\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 91/120 with 223 chunks\n",
      "Epoch 92/120 with 223 chunks\n",
      "Step: 61000\n",
      "\t * Smooth loss: 125.8440\n",
      "Epoch 93/120 with 223 chunks\n",
      "Step: 62000\n",
      "\t * Smooth loss: 125.7423\n",
      "Epoch 94/120 with 223 chunks\n",
      "Epoch 95/120 with 223 chunks\n",
      "Step: 63000\n",
      "\t * Smooth loss: 125.9311\n",
      "Epoch 96/120 with 223 chunks\n",
      "Step: 64000\n",
      "\t * Smooth loss: 125.7677\n",
      "Epoch 97/120 with 223 chunks\n",
      "Epoch 98/120 with 223 chunks\n",
      "Step: 65000\n",
      "\t * Smooth loss: 125.7356\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "t\n",
      "It cap shit you starting in a ball roct\n",
      "If you can't take it to something shit it get to me\n",
      "I'm han to the thing\n",
      "'Cause that this back and strunks don't wanna got your prosed a carta baby from on th\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 99/120 with 223 chunks\n",
      "Step: 66000\n",
      "\t * Smooth loss: 125.7411\n",
      "Epoch 100/120 with 223 chunks\n",
      "Epoch 101/120 with 223 chunks\n",
      "Step: 67000\n",
      "\t * Smooth loss: 125.5831\n",
      "Epoch 102/120 with 223 chunks\n",
      "Step: 68000\n",
      "\t * Smooth loss: 125.5714\n",
      "Epoch 103/120 with 223 chunks\n",
      "Epoch 104/120 with 223 chunks\n",
      "Step: 69000\n",
      "\t * Smooth loss: 125.5582\n",
      "Epoch 105/120 with 223 chunks\n",
      "Step: 70000\n",
      "\t * Smooth loss: 125.5884\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "le here\n",
      "I got an of the ways show up on the she step to the one of the fuckin' afre the corce the stret a crunk\n",
      "I think you know what I just don't know I the year in the swife there's stand of it in t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 106/120 with 223 chunks\n",
      "Epoch 107/120 with 223 chunks\n",
      "Step: 71000\n",
      "\t * Smooth loss: 125.5019\n",
      "Epoch 108/120 with 223 chunks\n",
      "Step: 72000\n",
      "\t * Smooth loss: 125.5378\n",
      "Epoch 109/120 with 223 chunks\n",
      "Epoch 110/120 with 223 chunks\n",
      "Step: 73000\n",
      "\t * Smooth loss: 125.3135\n",
      "Epoch 111/120 with 223 chunks\n",
      "Step: 74000\n",
      "\t * Smooth loss: 125.3167\n",
      "Epoch 112/120 with 223 chunks\n",
      "Epoch 113/120 with 223 chunks\n",
      "Step: 75000\n",
      "\t * Smooth loss: 125.1330\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "that give it\n",
      "You can't even dould trank the spit on you started the see the place you ain't been do that I'm a place is it the School ass\n",
      "So I was so get the coming to be the record and stran siff, th\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 114/120 with 223 chunks\n",
      "Step: 76000\n",
      "\t * Smooth loss: 125.3009\n",
      "Epoch 115/120 with 223 chunks\n",
      "Epoch 116/120 with 223 chunks\n",
      "Step: 77000\n",
      "\t * Smooth loss: 125.1841\n",
      "Epoch 117/120 with 223 chunks\n",
      "Step: 78000\n",
      "\t * Smooth loss: 125.2733\n",
      "Epoch 118/120 with 223 chunks\n",
      "Epoch 119/120 with 223 chunks\n",
      "Step: 79000\n",
      "\t * Smooth loss: 125.1133\n",
      "Epoch 120/120 with 223 chunks\n",
      "Step: 80000\n",
      "\t * Smooth loss: 125.1161\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "nild to all the some and I would get a close to rait and and grow to get and girl, was a fice the say now I'm any stable to the but the plicks and as is his cours of da sky what a head\n",
      "The scraice of \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 120\n",
    "batch_size = 16\n",
    "eta = 0.001\n",
    "smooth_loss = 0\n",
    "seq_length = 75\n",
    "losses = []\n",
    "\n",
    "min_L, max_L = 120, 150\n",
    "\n",
    "beta_1, beta_2, epsilon = 0.9, 0.999, 1e-8\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "vb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "vc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "vU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "vV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "vW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "vs = {'b': vb, 'c': vc, 'U': vU, 'V': vV, 'W': vW}\n",
    "\n",
    "step, epoch = 0, 0\n",
    "hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    L = random.randint(min_L, max_L + 1)\n",
    "    chunks = split_into_chunks(book_data, L)\n",
    "    random.shuffle(chunks)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} with {L} chunks\")\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        #print(f\"Processing chunk {idx+1}/{L}\")\n",
    "        e = 0\n",
    "        \n",
    "        while e < (len(chunk) - batch_size * seq_length):\n",
    "            X_batch = []\n",
    "            Y_batch = []\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                start_index = e + b * seq_length\n",
    "                X_chars = chunk[start_index:(start_index + seq_length)]\n",
    "                Y_chars = chunk[(start_index + 1):(start_index + seq_length + 1)]\n",
    "                X_batch.append(encode_string(X_chars))\n",
    "                Y_batch.append(encode_string(Y_chars))\n",
    "            \n",
    "            X_train = torch.stack(X_batch, dim=2)\n",
    "            Y_train = torch.stack(Y_batch, dim=2)\n",
    "            \n",
    "            A_train, H_train, P_train, hts = forward_batch(RNN, X_train, hprev)\n",
    "            loss = compute_loss_batch(Y_train, P_train)\n",
    "            grads, grads_clamped = backward_batch(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "            for k in ms.keys():\n",
    "                ms[k] = beta_1*ms[k] + (1 - beta_1)*grads_clamped[k]\n",
    "                vs[k] = beta_2*vs[k] + (1 - beta_2)*(grads_clamped[k]**2)\n",
    "                m_hat = ms[k]/(1 - beta_1**(step+1))\n",
    "                v_hat = vs[k]/(1 - beta_2**(step+1))\n",
    "                RNN[k] -= (eta/torch.sqrt(v_hat + epsilon))*m_hat\n",
    "            \n",
    "            if step == 0:\n",
    "                smooth_loss = loss\n",
    "            else:\n",
    "                smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "            losses.append(smooth_loss)\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                print(f\"Step: {step}\")\n",
    "                print(f\"\\t * Smooth loss: {smooth_loss:.4f}\")\n",
    "            if step % 5000 == 0:\n",
    "                _, s_syn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 200, 0.6)\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "                print(\"-\" * 100)\n",
    "            if step % 100000 == 0 and step > 0:\n",
    "                _, s_lsyn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 1000, 0.6)\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "                print(\"-\" * 100)\n",
    "            \n",
    "            e += batch_size * seq_length\n",
    "            step += 1\n",
    "            hprev = hts\n",
    "            \n",
    "        if e >= len(chunk) - batch_size * seq_length:\n",
    "            hprev = torch.zeros((m, batch_size), dtype=torch.double)  # Reset hidden state for new chunk\n",
    "        \n",
    "    epoch += 1\n",
    "\n",
    "# Save trained RNN\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
